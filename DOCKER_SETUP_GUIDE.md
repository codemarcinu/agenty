# ğŸ³ Docker Setup Guide - Option 3: PeÅ‚ny Stack

Przewodnik uruchomienia peÅ‚nego stacku Docker z integracjÄ… mini aplikacji konsolowych.

## ğŸ¯ Cel: Uruchomienie Option 3

```
Mini Aplikacje (host) â†’ Backend Container â†’ Ollama Container â†’ Modele AI
```

---

## ğŸ“‹ Krok 1: Sprawdzenie Åšrodowiska

```bash
# SprawdÅº Docker
docker --version
docker-compose --version

# SprawdÅº dostÄ™pne zasoby
docker system df
docker system info | grep -E "(Total Memory|CPUs)"
```

**Minimalne wymagania:**
- Docker Desktop aktywny
- 16GB+ RAM dostÄ™pne
- 50GB+ wolnego miejsca (modele AI sÄ… duÅ¼e)

---

## ğŸ”§ Krok 2: Przygotowanie Konfiguracji

### A) SprawdÅº obecnÄ… konfiguracjÄ™:
```bash
cd /home/marcin/Dokumenty/PROJEKT/my_assistant

# SprawdÅº docker-compose
ls -la config/docker/
cat config/docker/docker-compose.optimized.yaml | grep -A5 -B5 "OLLAMA_URL"
```

### B) Problem: Konflikt konfiguracji
```yaml
# W docker-compose.optimized.yaml (kontenery):
- OLLAMA_URL=http://ollama:11434     # â† Service name dla kontenerÃ³w

# W src/backend/settings.py (mini aplikacje):
OLLAMA_URL = "http://localhost:11434"  # â† Localhost dla hosta
```

### C) RozwiÄ…zanie: Port forwarding
Ollama container musi byÄ‡ dostÄ™pny z hosta na `localhost:11434`

---

## ğŸš€ Krok 3: Uruchomienie Stack

```bash
cd /home/marcin/Dokumenty/PROJEKT/my_assistant

# Uruchom peÅ‚ny stack
docker-compose -f config/docker/docker-compose.optimized.yaml up -d

# SprawdÅº status
docker-compose -f config/docker/docker-compose.optimized.yaml ps
```

**Oczekiwany output:**
```
NAME                STATUS              PORTS
foodsave-backend    Up 2 minutes        0.0.0.0:8000->8000/tcp
foodsave-ollama     Up 2 minutes        0.0.0.0:11434->11434/tcp  â† WaÅ¼ne!
foodsave-redis      Up 2 minutes        0.0.0.0:6379->6379/tcp
```

---

## ğŸ” Krok 4: Weryfikacja PoÅ‚Ä…czeÅ„

### A) Test Ollama z hosta:
```bash
# Test podstawowy
curl http://localhost:11434/api/version

# Powinno zwrÃ³ciÄ‡:
# {"version":"0.1.x"}
```

### B) SprawdÅº dostÄ™pne modele:
```bash
# Lista modeli w kontenerze
docker exec foodsave-ollama ollama list

# JeÅ›li puste, zaÅ‚aduj modele
docker exec foodsave-ollama ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M
docker exec foodsave-ollama ollama pull llama3.2:3b
```

### C) Test modelu:
```bash
# Test inferecji z hosta
curl http://localhost:11434/api/generate -d '{
  "model": "SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M",
  "prompt": "Napisz krÃ³tkie podsumowanie sztucznej inteligencji",
  "stream": false
}' | jq .response
```

---

## ğŸ® Krok 5: Uruchomienie Mini Aplikacji

```bash
# PowrÃ³Ä‡ do gÅ‚Ã³wnego katalogu
cd /home/marcin/Dokumenty/PROJEKT/my_assistant

# Uruchom menedÅ¼er konsoli
python console_manager.py

# W menu wybierz:
# Opcja 11: Diagnostyka systemu (sprawdzi poÅ‚Ä…czenia)
```

**Oczekiwany rezultat diagnostyki:**
```
ğŸ©º DIAGNOSTYKA SYSTEMU
=====================
ğŸ” Sprawdzam dostÄ™pnoÅ›Ä‡ skryptÃ³w...
âœ… Gmail Inbox Zero Agent Test: OK
âœ… General Chat & Search Agent Test: OK
âœ… Anti-Hallucination System Test: OK

ğŸ” Sprawdzam zaleÅ¼noÅ›ci Python...
âœ… asyncio: Asynchroniczne operacje
âœ… json: ObsÅ‚uga JSON
âœ… logging: System logowania

ğŸ” Sprawdzam system plikÃ³w...
âœ… Uprawnienia zapisu: OK

ğŸ“Š PODSUMOWANIE DIAGNOSTYKI:
âœ… System gotowy do pracy - nie znaleziono problemÃ³w
```

---

## ğŸ§ª Krok 6: Test PeÅ‚nej Integracji

### A) Test Gmail Inbox Zero:
```bash
# W console_manager.py wybierz opcjÄ™ 1
# NastÄ™pnie opcja 1: Analizuj przykÅ‚adowy email
# Wybierz typ 1: Email biznesowy
```

**Oczekiwany output:**
```
ğŸ¤” Przetwarzam pytanie: [email content]

âœ… OdpowiedÅº agenta:
Status: Sukces
Czas przetwarzania: 2.34s
Tekst: Analiza emaila zakoÅ„czona. Priorytet: high
Sugerowane labele: ['Praca', 'WaÅ¼ne']
Powinno byÄ‡ zarchiwizowane: false
Wymaga odpowiedzi: true
Priorytet: high
PewnoÅ›Ä‡: 0.85
```

### B) Test General Chat:
```bash
# W console_manager.py wybierz opcjÄ™ 2
# NastÄ™pnie opcja 1: Zadaj pytanie ogÃ³lne
# Wpisz: "Co to jest sztuczna inteligencja?"
```

**Oczekiwany output:**
```
ğŸ¤” Przetwarzam pytanie: Co to jest sztuczna inteligencja?

âœ… OdpowiedÅº agenta:
Status: Sukces
Czas przetwarzania: 3.12s
Tekst: [SzczegÃ³Å‚owa odpowiedÅº o AI wygenerowana przez Bielik-11B]
PewnoÅ›Ä‡: 0.92
Model uÅ¼yty: SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M
```

---

## ğŸ”§ RozwiÄ…zywanie ProblemÃ³w

### Problem 1: Ollama nie odpowiada
```bash
# SprawdÅº logi
docker logs foodsave-ollama

# SprawdÅº czy port jest otwarty
netstat -tlnp | grep 11434

# Restart Ollama
docker restart foodsave-ollama
```

### Problem 2: Model nie znaleziony
```bash
# SprawdÅº modele w kontenerze
docker exec foodsave-ollama ollama list

# ZaÅ‚aduj model
docker exec foodsave-ollama ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M

# SprawdÅº rozmiar modelu (moÅ¼e byÄ‡ duÅ¼y!)
docker exec foodsave-ollama du -sh /root/.ollama/models/
```

### Problem 3: Backend nie Å‚Ä…czy siÄ™ z Ollama
```bash
# SprawdÅº logi backendu
docker logs foodsave-backend | grep -i ollama

# SprawdÅº sieÄ‡ Docker
docker network inspect foodsave_foodsave-network

# Test poÅ‚Ä…czenia z backend do ollama
docker exec foodsave-backend curl http://ollama:11434/api/version
```

### Problem 4: Import errors w mini aplikacjach
```bash
# SprawdÅº czy jesteÅ› w gÅ‚Ã³wnym katalogu
pwd
# Powinno byÄ‡: /home/marcin/Dokumenty/PROJEKT/my_assistant

# SprawdÅº strukturÄ™
ls -la src/backend/agents/

# SprawdÅº Python path
python -c "import sys; print('\n'.join(sys.path))"
```

---

## ğŸ“Š Monitoring Stack

### SprawdÅº zasoby:
```bash
# CPU i RAM wszystkich kontenerÃ³w
docker stats

# SzczegÃ³Å‚y konkretnego kontenera
docker stats foodsave-ollama --no-stream

# Logs w czasie rzeczywistym
docker logs -f foodsave-backend
```

### Health checks:
```bash
# Status wszystkich serwisÃ³w
docker-compose -f config/docker/docker-compose.optimized.yaml ps

# Health check konkretnego serwisu
docker inspect foodsave-ollama | jq '.[].State.Health'
```

---

## ğŸ¯ Optymalizacja WydajnoÅ›ci

### Dla RTX 3060:
```yaml
# W docker-compose.optimized.yaml juÅ¼ skonfigurowane:
environment:
  - OLLAMA_GPU_LAYERS=43          # UÅ¼yj GPU
  - OLLAMA_NUM_PARALLEL=4         # 4 rÃ³wnolegÅ‚e requesty
  - OLLAMA_MAX_LOADED_MODELS=3    # Max 3 modele w pamiÄ™ci
  - OLLAMA_FLASH_ATTENTION=1      # Flash attention
  - OLLAMA_GPU_MEMORY_UTILIZATION=0.85  # 85% VRAM
```

### Monitorowanie GPU:
```bash
# SprawdÅº uÅ¼ycie GPU
nvidia-smi

# W kontenerze Ollama
docker exec foodsave-ollama nvidia-smi
```

---

## ğŸ”„ Kompletny Workflow

### 1. Start Stack:
```bash
cd /home/marcin/Dokumenty/PROJEKT/my_assistant
docker-compose -f config/docker/docker-compose.optimized.yaml up -d
```

### 2. ZaÅ‚aduj Modele:
```bash
docker exec foodsave-ollama ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M
docker exec foodsave-ollama ollama pull llama3.2:3b
```

### 3. SprawdÅº Status:
```bash
docker-compose -f config/docker/docker-compose.optimized.yaml ps
curl http://localhost:11434/api/version
curl http://localhost:8000/health
```

### 4. Uruchom Mini Aplikacje:
```bash
python console_manager.py
```

### 5. Test FunkcjonalnoÅ›ci:
- Gmail Inbox Zero (opcja 1)
- General Chat (opcja 2)  
- Anti-Hallucination (opcja 3)

### 6. Stop Stack (gdy skoÅ„czysz):
```bash
docker-compose -f config/docker/docker-compose.optimized.yaml down
```

---

## ğŸ‰ Sukces!

Po wykonaniu tych krokÃ³w bÄ™dziesz miaÅ‚:

âœ… **PeÅ‚ny stack Docker** z backend, Ollama, Redis  
âœ… **Modele AI** gotowe do uÅ¼ycia (Bielik-11B, Llama3.2)  
âœ… **Mini aplikacje** z peÅ‚nÄ… integracjÄ… AI  
âœ… **GPU acceleration** dla RTX 3060  
âœ… **Production-ready** konfiguracjÄ™  

**Gotowe do testowania wszystkich funkcji agentowych! ğŸš€**