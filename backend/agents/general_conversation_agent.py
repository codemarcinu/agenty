"""
General Conversation Agent

Agent obs≈ÇugujƒÖcy swobodne konwersacje na dowolny temat z wykorzystaniem:
- RAG (Retrieval-Augmented Generation) dla wiedzy z dokument√≥w
- Wyszukiwania internetowego (DuckDuckGo, Perplexity) dla aktualnych informacji
- Bielika jako g≈Ç√≥wnego modelu jƒôzykowego
"""

import asyncio
from collections.abc import AsyncGenerator
import logging
import re
import time
from typing import Any

import numpy as np
from sqlalchemy.ext.asyncio import AsyncSession

from agents.base_agent import BaseAgent
from agents.interfaces import AgentResponse
from agents.tools import get_current_date
from core.anti_hallucination_decorator import (
    AntiHallucinationConfig,
    with_anti_hallucination,
)
from core.anti_hallucination_system import ValidationLevel
from core.cache_manager import cached_async, internet_cache, rag_cache
from core.decorators import handle_exceptions
from core.hybrid_llm_client import ModelComplexity, hybrid_llm_client
from core.performance_monitor import performance_monitor

# Lazy loading dla ciƒô≈ºkich import√≥w
_rag_processor = None
_rag_integration = None
_vector_store = None
_web_search = None
_mmlw_client = None

def _get_rag_processor():
    global _rag_processor
    if _rag_processor is None:
        from core.rag_document_processor import RAGDocumentProcessor
        _rag_processor = RAGDocumentProcessor()
    return _rag_processor

def _get_rag_integration():
    global _rag_integration
    if _rag_integration is None:
        from core.rag_integration import RAGDatabaseIntegration
        _rag_integration = RAGDatabaseIntegration(_get_rag_processor())
    return _rag_integration

def _get_vector_store():
    global _vector_store
    if _vector_store is None:
        from core.vector_store import vector_store
        _vector_store = vector_store
    return _vector_store

def _get_web_search():
    global _web_search
    if _web_search is None:
        from integrations.web_search import web_search
        _web_search = web_search
    return _web_search

def _get_mmlw_client():
    global _mmlw_client
    if _mmlw_client is None:
        from core.mmlw_embedding_client import mmlw_client
        _mmlw_client = mmlw_client
    return _mmlw_client

logger = logging.getLogger(__name__)


class GeneralConversationAgent(BaseAgent):
    """Agent do obs≈Çugi swobodnych konwersacji
    z wykorzystaniem RAG i wyszukiwania internetowego."""

    def __init__(
        self,
        name: str = "GeneralConversationAgent",
        timeout=None,
        plugins=None,
        initial_state=None,
        **kwargs,
    ) -> None:
        super().__init__(name, **kwargs)
        self.timeout = timeout
        self.plugins = plugins or []
        self.initial_state = initial_state or {}
        self.rag_processor = _get_rag_processor()
        self.rag_integration = _get_rag_integration()
        self.description = "Agent do obs≈Çugi swobodnych konwersacji z wykorzystaniem RAG i wyszukiwania internetowego"

    @with_anti_hallucination(
        AntiHallucinationConfig(
            validation_level=ValidationLevel.MODERATE, log_validation=True
        )
    )
    @handle_exceptions(max_retries=2)
    async def process(
        self, input_data: dict[str, Any], db: AsyncSession | None = None
    ) -> AgentResponse:
        """Process user query with RAG and internet search in parallel"""
        start_time = time.time()
        
        try:
            query = self._extract_query_from_input(input_data)
            session_id = input_data.get("session_id", "")
            use_perplexity = input_data.get("use_perplexity", False)
            use_bielik = input_data.get("use_bielik", True)

            if not query:
                duration = time.time() - start_time
                performance_monitor.record_operation("chat_response", duration, {"error": "no_query"})
                return AgentResponse(
                    success=False,
                    error="No query provided in input_data",
                    data={"available_fields": list(input_data.keys())},
                    text="",
                )

            # üöÄ EARLY EXIT: Sprawd≈∫ czy to proste zapytanie
            if self._is_simple_query(query):
                logger.info(f"Early exit for simple query: {query}")
                performance_monitor.record_early_exit()
                simple_response = self._generate_simple_response(query)
                duration = time.time() - start_time
                performance_monitor.record_operation("chat_response", duration, {"early_exit": True})
                return AgentResponse(
                    success=True,
                    text=simple_response,
                    data={
                        "query": query,
                        "used_rag": False,
                        "used_internet": False,
                        "early_exit": True,
                        "session_id": session_id,
                    },
                    request_id=input_data.get("request_id"),
                )

            # Check if this is a pantry query and use pantry tools
            logger.info(
                f"[GeneralConversationAgent] Checking pantry detection for query: {query}"
            )
            pantry_detected = self._should_use_pantry_tools(query)
            logger.info(
                f"[GeneralConversationAgent] Pantry detection result: {pantry_detected}"
            )
            if pantry_detected and db is not None:
                logger.info(
                    f"[GeneralConversationAgent] Detected pantry query: {query}"
                )
                pantry_result = await self._execute_pantry_tools(query, db)
                if pantry_result:
                    duration = time.time() - start_time
                    performance_monitor.record_operation("chat_response", duration, {"pantry_tools": True})
                    return AgentResponse(
                        success=True,
                        text=pantry_result,
                        data={
                            "query": query,
                            "used_pantry_tools": True,
                            "used_rag": False,
                            "used_internet": False,
                            "rag_confidence": 0.0,
                            "use_perplexity": use_perplexity,
                            "use_bielik": use_bielik,
                            "session_id": session_id,
                        },
                    )

            # Continue with normal processing
            logger.info(
                f"[GeneralConversationAgent] Processing query: {query}... "
                f"use_perplexity={use_perplexity}, use_bielik={use_bielik}"
            )

            # Sprawd≈∫ czy to pytanie o datƒô/czas - natychmiastowa odpowied≈∫
            if self._is_date_query(query):
                logger.info(f"Detected date query: {query}")
                date_response = get_current_date()
                return AgentResponse(
                    success=True,
                    text=date_response,
                    data={
                        "query": query,
                        "used_rag": False,
                        "used_internet": False,
                        "rag_confidence": 0.0,
                        "use_perplexity": use_perplexity,
                        "use_bielik": use_bielik,
                        "session_id": session_id,
                        "is_date_query": True,
                    },
                )

            # Debug prints
            logger.debug(f"Input data: {input_data}")
            logger.debug(
                "Starting GeneralConversationAgent.process with parallel processing"
            )

            # Uruchom r√≥wnolegle pobieranie kontekstu z RAG i internetu
            rag_task = asyncio.create_task(self._get_rag_context(query))
            internet_task = asyncio.create_task(
                self._get_internet_context(query, use_perplexity)
            )

            # Czekaj na zako≈Ñczenie obu zada≈Ñ
            rag_result, internet_context = await asyncio.gather(rag_task, internet_task)
            rag_context, rag_confidence = rag_result

            logger.info(f"RAG context confidence: {rag_confidence}")
            logger.info(f"Internet search completed: {bool(internet_context)}")

            # Wygeneruj odpowied≈∫ z wykorzystaniem wszystkich ≈∫r√≥de≈Ç
            logger.debug("Generating response with combined context")
            response = await self._generate_response(
                query, rag_context, internet_context, use_perplexity, use_bielik
            )
            logger.info(
                f"[GeneralConversationAgent] Generated response before post-processing: '{response[:100]}...'"
            )

            # Check if response contains LLM fallback message and provide better response
            llm_fallback_message = (
                "I'm sorry, but I'm currently unable to process your request. "
                "Please try again later."
            )
            if response and llm_fallback_message in response:
                logger.info(
                    "Detected LLM fallback message, providing graceful response"
                )
                response = "Przepraszam, obecnie mam trudno≈õci z przetworzeniem Twojego zapytania. Spr√≥buj zadaƒá je w inny spos√≥b."

            # If no response could be generated, provide meaningful fallback
            if not response or not response.strip():
                logger.warning(f"Empty response generated for query: {query}")
                if self._is_weather_query(query):
                    response = (
                        "Nie mogƒô obecnie sprawdziƒá pogody. Spr√≥buj ponownie za chwilƒô."
                    )
                elif self._is_greeting(query):
                    response = "Cze≈õƒá! Jak mogƒô Ci pom√≥c?"
                else:
                    response = "Przepraszam, nie jestem w stanie odpowiedzieƒá na to pytanie w tym momencie. Mo≈ºesz spr√≥bowaƒá zadaƒá je ponownie?"

            # --- POST-PROCESSING ANTI-HALLUCINATION FILTER ---
            # Zaawansowany filtr z fuzzy matching i detekcjƒÖ wzorc√≥w halucynacji
            def contains_name_fuzzy(query_text: str, response_text: str) -> bool:
                """Sprawdza czy odpowied≈∫ zawiera imiƒô/nazwisko z query.
                Fuzzy match.
                Zwraca True, je≈õli znaleziono dopasowanie.
                UWAGA: Zablokowane dla pyta≈Ñ o w≈Çasne imiƒô u≈ºytkownika.
                """
                
                # Sprawd≈∫ czy to pytanie o w≈Çasne imiƒô
                own_name_patterns = [
                    r"jak mam na imiƒô",
                    r"jak siƒô nazywam",
                    r"jakie jest moje imiƒô",
                    r"mam na imiƒô",
                    r"nazywam siƒô"
                ]
                
                for pattern in own_name_patterns:
                    if re.search(pattern, query_text.lower()):
                        return False  # Nie blokuj odpowiedzi dla pyta≈Ñ o w≈Çasne imiƒô
                # WyciƒÖgnij potencjalne imiona/nazwiska z query
                name_pattern = (
                    r"\b[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\b"
                )
                names_in_query = re.findall(name_pattern, query_text)

                # Lista typowych polskich imion do sprawdzenia
                polish_names = [
                    "jan",
                    "janusz",
                    "piotr",
                    "andrzej",
                    "tomasz",
                    "marek",
                    "micha≈Ç",
                    "krzysztof",
                    "wojciech",
                    "anna",
                    "maria",
                    "katarzyna",
                    "ma≈Çgorzata",
                    "agnieszka",
                    "barbara",
                    "ewa",
                    "el≈ºbieta",
                    "joanna",
                    "kamil",
                    "mateusz",
                    "dawid",
                    "jakub",
                    "szymon",
                    "filip",
                    "miko≈Çaj",
                    "bartosz",
                    "adrian",
                    "natalia",
                    "aleksandra",
                    "karolina",
                    "paulina",
                    "monika",
                    "sylwia",
                    "iwona",
                    "dorota",
                    "renata",
                ]

                for name in names_in_query:
                    # Sprawd≈∫ czy imiƒô lub nazwisko wystƒôpuje w odpowiedzi
                    first_name, last_name = name.split()
                    if (
                        first_name.lower() in response_text.lower()
                        or last_name.lower() in response_text.lower()
                    ):
                        return True

                # Sprawd≈∫ czy odpowied≈∫ zawiera jakiekolwiek polskie imiƒô
                # (ale tylko je≈õli nie jest to odpowied≈∫ na pytanie o w≈Çasne imiƒô)
                response_lower = response_text.lower()
                for polish_name in polish_names:
                    if polish_name in response_lower and re.search(
                        r"\b" + polish_name + r"\b", response_lower
                    ):
                        # Dodatkowe sprawdzenie - czy to nie odpowied≈∫ typu "Nazywasz siƒô Marcin"
                        context_patterns = [
                            "nazywasz siƒô", "masz na imiƒô", "twoje imiƒô to",
                            "jeste≈õ", "to ty"
                        ]
                        if any(pattern in response_lower for pattern in context_patterns):
                            return False  # Nie blokuj je≈õli to odpowied≈∫ o u≈ºytkowniku
                        return True

                return False

            def is_product_query(query_text: str) -> bool:
                """Sprawdza czy query dotyczy produktu.
                Zwraca True, je≈õli zapytanie dotyczy produktu.
                """
                product_keywords = [
                    "telefon",
                    "smartfon",
                    "laptop",
                    "komputer",
                    "tablet",
                    "kamera",
                    "s≈Çuchawki",
                    "specyfikacja",
                    "specyfikacje",
                    "parametry",
                    "cechy",
                    "funkcje",
                    "wyposa≈ºenie",
                    "samsung",
                    "iphone",
                    "xiaomi",
                    "huawei",
                    "lenovo",
                    "dell",
                    "hp",
                    "asus",
                ]
                query_lower = query_text.lower()
                return any(keyword in query_lower for keyword in product_keywords)

            def is_person_query(query_text: str) -> bool:
                """Sprawdza czy query dotyczy osoby - ulepszona logika kontekstowa"""
                query_lower = query_text.lower()

                # Wykluczenia - pytania kt√≥re na pewno NIE dotyczƒÖ os√≥b
                exclusion_patterns = [
                    r"cena\s+\w+",  # "cena bitcoina", "cena akcji"
                    r"aktualna\s+cena",
                    r"ile\s+kosztuje",
                    r"warto≈õƒá\s+\w+",
                    r"kurs\s+\w+",
                    r"notowania",
                    r"gie≈Çda",
                    r"bitcoin",
                    r"ethereum",
                    r"kryptowalut",
                    r"pogoda",
                    r"temperatura",
                    r"przepis\s+na",
                    r"jak\s+zrobiƒá",
                    r"sk≈Çadniki",
                    r"technologia",
                    r"komputer",
                    r"smartfon",
                ]

                for pattern in exclusion_patterns:
                    if re.search(pattern, query_lower):
                        return False

                # Silne wska≈∫niki osoby - tylko te jednoznaczne
                strong_person_indicators = [
                    "kto",
                    "kim",
                    "biografia",
                    "≈ºyciorys",
                    "urodzi≈Ç siƒô",
                    "urodzi≈Ça siƒô",
                    "zmar≈Ç",
                    "zmar≈Ça",
                ]

                for indicator in strong_person_indicators:
                    if indicator in query_lower:
                        return True

                # S≈Çabsze wska≈∫niki - wymagajƒÖ dodatkowego kontekstu
                weak_person_indicators = [
                    "naukowiec",
                    "profesor",
                    "doktor",
                    "in≈ºynier",
                    "lekarz",
                    "artysta",
                    "polski",
                    "polska",
                    "polak",
                    "polka",
                ]

                # Sprawd≈∫ czy sƒÖ imiona/nazwiska w zapytaniu
                has_name_pattern = bool(
                    re.search(
                        r"\b[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\b",
                        query_text,
                    )
                )

                # S≈Çabe wska≈∫niki liczymy tylko je≈õli jest te≈º wzorzec imienia
                if has_name_pattern:
                    for indicator in weak_person_indicators:
                        if indicator in query_lower:
                            return True

                return False

            def contains_hallucination_patterns(response_text: str) -> bool:
                """Sprawdza czy odpowied≈∫ zawiera typowe wzorce halucynacji
                (czyli czy odpowied≈∫ zawiera typowe wzorce halucynacji)"""
                hallucination_patterns = [
                    # Wzorce dla os√≥b
                    r"by≈Ç\s+wybitnym",
                    r"urodzi≈Ç\s+siƒô",
                    r"zmar≈Ç\s+w",
                    r"jego\s+najwa≈ºniejsze\s+osiƒÖgniƒôcia",
                    r"by≈Ç\s+polskim\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+",
                    r"studia\s+na\s+uniwersytecie",
                    r"profesor\s+uniwersytetu",
                    r"cz≈Çonek\s+akademii",
                    r"prace\s+naukowe",
                    r"wynalazca",
                    r"chemik",
                    r"fizyk",
                    r"matematyk",
                    # Wzorce dla produkt√≥w
                    r"specyfikacja",
                    r"ekran\s+o\s+przekƒÖtnej",
                    r"procesor",
                    r"bateria\s+ma\s+pojemno≈õƒá",
                    r"posiada\s+procesor",
                    r"wyposa≈ºony\s+jest\s+w",
                    r"ram\s+\d+\s+gb",
                    r"pamiƒôƒá\s+wewnƒôtrzna",
                    r"rozdzielczo≈õƒá",
                    r"pojemno≈õƒá\s+baterii",
                    # KRYTYCZNE: Wzorce dla przepis√≥w (halucynacje)
                    r"\d+\.\s*[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+[w]?\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+[przez]\s+\d+\s+minut",
                    r"ugotuj\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+\s+w\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+wodzie",
                    r"dodaj\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+\s+do\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+patelni",
                    r"pokr√≥j\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+\s+w\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+kostkƒô",
                    r"posyp\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+parmezanem",
                    r"przepis:",
                    r"kroki:",
                    r"instrukcja:",
                    # KRYTYCZNE: Wzorce dla wydarze≈Ñ (halucynacje)
                    r"obchodzony\s+jest\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+dzie≈Ñ",
                    r"upamiƒôtnia\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+wydarzenie",
                    r"g≈Ç√≥wne\s+obchody\s+odbywajƒÖ\s+siƒô",
                    r"uroczysto≈õci\s+pa≈Ñstwowych",
                    r"manifestacji\s+patriotycznych",
                    r"w\s+[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\s]+na\s+placu",
                    r"z\s+udzia≈Çem\s+w≈Çadz",
                    # Wzorce dla szczeg√≥≈Çowych informacji bez ≈∫r√≥de≈Ç
                    r"\d{4}\s+roku",
                    r"w\s+\d{4}\s+roku",
                    r"od\s+\d{4}\s+do\s+\d{4}",
                    r"w\s+latach\s+\d{4}-\d{4}",
                ]

                for pattern in hallucination_patterns:
                    if re.search(pattern, response_text, re.IGNORECASE):
                        return True
                return False

            def is_known_person(query_text: str) -> bool:
                """Sprawdza czy query dotyczy znanej, zweryfikowanej osoby
                (czyli czy zapytanie dotyczy znanej osoby)"""
                known_persons = [
                    # Politycy i osoby publiczne
                    "andrzej duda",
                    "prezydent polski",
                    "prezydent polski",
                    "donald tusk",
                    "mateusz morawiecki",
                    "w≈Çadys≈Çaw kosiniak-kamysz",
                    "szymon ho≈Çownia",
                    "krzysztof bosak",
                    "robert biedro≈Ñ",
                    # Znane postacie historyczne
                    "j√≥zef pi≈Çsudski",
                    "lech wa≈Çƒôsa",
                    "jan pawe≈Ç ii",
                    "miko≈Çaj kopernik",
                    "maria sk≈Çodowska",
                    "fryderyk chopin",
                    "adam mickiewicz",
                    "juliusz s≈Çowacki",
                    "henryk sienkiewicz",
                    # Aktualne osoby publiczne
                    "robert lewandowski",
                    "iga ≈õwiƒÖtek",
                    "andrzej wajda",
                    "roman pola≈Ñski",
                    # Dodatkowe warianty
                    "prezydent",
                    "prezydenta",
                    "prezydentem",
                    "prezydentowi",
                ]
                query_lower = query_text.lower()

                # Sprawd≈∫ czy query zawiera s≈Çowo "prezydent" + "polski/polska"
                if "prezydent" in query_lower and (
                    "polski" in query_lower or "polska" in query_lower
                ):
                    return True

                return any(person in query_lower for person in known_persons)

            def is_future_event(query_text: str) -> bool:
                """Sprawdza czy query dotyczy wydarzenia z przysz≈Ço≈õci
                (czyli czy zapytanie dotyczy przysz≈Çych wydarze≈Ñ)"""
                future_patterns = [
                    r"\b202[5-9]\b",  # Lata 2025-2029
                    r"\b20[3-9][0-9]\b",  # Lata 2030-2099
                    r"\bprzysz≈Ço≈õci\b",
                    r"\bprzysz≈Çym\b",
                    r"\bprzysz≈Ça\b",
                    r"\bzaplanowane\b",
                    r"\bodbƒôdzie\b",
                    r"\bodbƒôdƒÖ\b",
                ]
                query_lower = query_text.lower()
                return any(
                    re.search(pattern, query_lower) for pattern in future_patterns
                )

            def is_recipe_query(query_text: str) -> bool:
                """Sprawdza czy query dotyczy przepisu"""
                recipe_patterns = [
                    r"przepis",
                    r"przygotuj",
                    r"ugotuj",
                    r"upiecz",
                    r"zr√≥b",
                    r"danie",
                    r"obiad",
                    r"≈õniadanie",
                    r"kolacja",
                    r"posi≈Çek",
                    r"jedzenie",
                    r"kuchnia",
                    r"gotowanie",
                ]
                query_lower = query_text.lower()
                return any(
                    re.search(pattern, query_lower) for pattern in recipe_patterns
                )

            def is_event_query(query_text: str) -> bool:
                """Sprawdza czy query dotyczy wydarzenia"""
                event_patterns = [
                    r"wydarzenie",
                    r"obchody",
                    r"uroczysto≈õƒá",
                    r"≈õwiƒôto",
                    r"dzie≈Ñ",
                    r"festiwal",
                    r"koncert",
                    r"wystawa",
                    r"konferencja",
                    r"spotkanie",
                    r"ceremonia",
                    r"parada",
                ]
                query_lower = query_text.lower()
                return any(
                    re.search(pattern, query_lower) for pattern in event_patterns
                )

            def _validate_response_against_context(
                response: str, context: str
            ) -> dict[str, Any]:
                """Waliduje odpowied≈∫ przeciwko dostƒôpnemu kontekstowi"""
                if not context:
                    return {"is_valid": False, "reason": "Brak kontekstu"}

                # Sprawd≈∫ czy odpowied≈∫ zawiera szczeg√≥≈Çowe informacje bez potwierdzenia w kontek≈õcie
                response_lower = response.lower()
                context_lower = context.lower()

                # Sprawd≈∫ czy odpowied≈∫ zawiera przepis bez sk≈Çadnik√≥w w kontek≈õcie
                if any(
                    word in response_lower
                    for word in ["przepis", "ugotuj", "dodaj", "pokr√≥j"]
                ) and not any(
                    word in context_lower
                    for word in [
                        "sk≈Çadnik",
                        "produkt",
                        "makaron",
                        "kurczak",
                        "pomidor",
                    ]
                ):
                    return {
                        "is_valid": False,
                        "reason": "Przepis bez dostƒôpnych sk≈Çadnik√≥w",
                    }

                # Sprawd≈∫ czy odpowied≈∫ zawiera wydarzenie bez daty w kontek≈õcie
                if any(
                    word in response_lower
                    for word in ["obchodzony", "upamiƒôtnia", "g≈Ç√≥wne obchody"]
                ) and not any(
                    word in context_lower
                    for word in ["data", "dzisiaj", "jutro", "wczoraj"]
                ):
                    return {
                        "is_valid": False,
                        "reason": "Wydarzenie bez potwierdzonej daty",
                    }

                # Sprawd≈∫ czy odpowied≈∫ zawiera szczeg√≥≈Çowe kroki bez instrukcji w kontek≈õcie
                if re.search(r"\d+\.\s*[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]", response):
                    if not any(
                        word in context_lower
                        for word in ["instrukcja", "krok", "spos√≥b", "metoda"]
                    ):
                        return {
                            "is_valid": False,
                            "reason": "Szczeg√≥≈Çowe kroki bez instrukcji w kontek≈õcie",
                        }

                return {"is_valid": True, "reason": "Odpowied≈∫ potwierdzona kontekstem"}

            # Sprawd≈∫ czy odpowied≈∫ zawiera halucynacje
            # rag_context to tuple (str, float), wiƒôc bierzemy tylko tekst
            rag_text = (
                rag_context[0]
                if isinstance(rag_context, tuple)
                else (rag_context or "")
            )
            context_text = rag_text + (internet_context or "")
            
            # Sprawd≈∫ czy to pytanie o osobƒô publicznƒÖ - je≈õli tak, nie blokuj wyszukiwania
            public_figure_patterns = [
                r"karol nawrocki",
                r"wybory prezydenckie", 
                r"prezydent.*pol",
                r"kandydat.*prezydent",
                r"andrzej duda",
                r"donald tusk"
            ]
            
            is_public_figure_query = any(
                re.search(pattern, query.lower()) for pattern in public_figure_patterns
            )

            # 1. Sprawd≈∫ fuzzy matching dla nazwisk
            fuzzy_match = contains_name_fuzzy(query, response)
            logger.info(
                f"[GeneralConversationAgent] Fuzzy name check: query='{query}', response='{response[:50]}...', fuzzy_match={fuzzy_match}"
            )
            
            # Dla os√≥b publicznych - zawsze u≈ºyj wyszukiwania internetowego
            if fuzzy_match and is_public_figure_query:
                logger.info(f"Post-processing: Wykryte pytanie o osobƒô publicznƒÖ: {query}")
                # U≈ºyj search agent dla os√≥b publicznych
                search_agent_result = await self._use_search_agent_for_query(
                    query, use_perplexity, use_bielik, session_id
                )
                if search_agent_result:
                    return search_agent_result
                    
            elif fuzzy_match:
                if is_known_person(query):
                    logger.info("Post-processing: Pomijam znanƒÖ osobƒô: " f"{query}")
                elif not any(
                    name in context_text
                    for name in re.findall(
                        r"\b[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\b",
                        query,
                    )
                ):
                    # Sprawd≈∫ czy zapytanie wymaga aktualnych danych przed zwr√≥ceniem odpowiedzi o osobie
                    current_data_indicators = [
                        "aktualna cena",
                        "najnowsza wiadomo≈õƒá",
                        "dzisiaj",
                        "obecnie",
                        "najnowsze informacje",
                        "aktualne dane",
                        "obecna sytuacja",
                        "ostatnie wiadomo≈õci",
                        "co siƒô dzieje",
                        "naj≈õwie≈ºsze",
                    ]

                    query_lower = query.lower()
                    should_search_current = any(
                        indicator in query_lower
                        for indicator in current_data_indicators
                    )

                    if should_search_current:
                        logger.info(
                            f"[GeneralConversationAgent] Wykryto fuzzy name ale zapytanie o aktualnƒÖ informacjƒô, prze≈ÇƒÖczam na SearchAgent: {query}"
                        )
                        try:
                            from agents.search_agent import SearchAgent

                            search_agent = SearchAgent()
                            search_input = {"query": query, "max_results": 3}
                            search_response = await search_agent.process(search_input)
                            if search_response and search_response.text:
                                return AgentResponse(
                                    success=True,
                                    text=f"[Wynik wyszukiwania internetowego]\n{search_response.text}",
                                    data={
                                        "query": query,
                                        "used_rag": bool(rag_context),
                                        "used_internet": True,
                                        "used_search_agent": True,
                                        "rag_confidence": 0.0,
                                        "use_perplexity": use_perplexity,
                                        "use_bielik": use_bielik,
                                        "session_id": session_id,
                                        "anti_hallucination": True,
                                        "trigger": "fuzzy_name_with_current_data",
                                    },
                                )
                        except Exception as e:
                            logger.error(
                                f"[GeneralConversationAgent] B≈ÇƒÖd podczas wywo≈Çania SearchAgent w fuzzy name handler: {e}"
                            )

                    logger.info(
                        "Post-processing: Wykryto halucynacjƒô nazwiska w query: "
                        f"{query}"
                    )
                    return AgentResponse(
                        success=True,
                        text="Nie mam informacji o tej osobie.",
                        data={
                            "query": query,
                            "used_rag": bool(rag_context),
                            "used_internet": False,
                            "used_search_agent": False,
                            "rag_confidence": 0.0,
                            "use_perplexity": use_perplexity,
                            "use_bielik": use_bielik,
                            "session_id": session_id,
                            "anti_hallucination": True,
                            "trigger": "fuzzy_name_match",
                        },
                    )

            # 2. Sprawd≈∫ wzorce halucynacji w odpowiedzi - NAPRAWIONA LOGIKA
            hallucination_patterns_match = contains_hallucination_patterns(response)
            logger.info(
                f"[GeneralConversationAgent] Hallucination patterns check: query='{query}', response='{response[:50]}...', patterns_match={hallucination_patterns_match}"
            )
            if hallucination_patterns_match:
                # Sprawd≈∫ czy kontekst zawiera informacje potwierdzajƒÖce odpowied≈∫
                context_validation = _validate_response_against_context(
                    response, context_text
                )

                if not context_validation["is_valid"]:
                    logger.info(
                        f"Post-processing: Wykryto wzorce halucynacji w odpowiedzi. Brak potwierdzenia w kontek≈õcie: {context_validation['reason']}"
                    )

                    # Sprawd≈∫ czy powinni≈õmy prze≈ÇƒÖczyƒá na SearchAgent zamiast zwracaƒá og√≥lnƒÖ odpowied≈∫
                    should_search = False

                    # Sprawd≈∫ czy zapytanie wymaga aktualnych danych
                    current_data_indicators = [
                        "aktualna cena",
                        "najnowsza wiadomo≈õƒá",
                        "dzisiaj",
                        "obecnie",
                        "najnowsze informacje",
                        "aktualne dane",
                        "obecna sytuacja",
                        "ostatnie wiadomo≈õci",
                        "co siƒô dzieje",
                        "naj≈õwie≈ºsze",
                    ]

                    query_lower = query.lower()
                    for indicator in current_data_indicators:
                        if indicator in query_lower:
                            should_search = True
                            break

                    # Sprawd≈∫ czy to nie jest typowe zapytanie o osobƒô (bez danych aktualnych)
                    # Ale je≈õli ju≈º wykryli≈õmy potrzebƒô wyszukiwania aktualnych danych, nie blokuj
                    if is_person_query(query) and not should_search:
                        should_search = False

                    logger.info(
                        f"[GeneralConversationAgent] Analiza zapytania: should_search={should_search}, is_person_query={is_person_query(query)}, query='{query}'"
                    )

                    if should_search:
                        logger.info(
                            "[GeneralConversationAgent] Halucynacja wykryta, ale zapytanie wymaga aktualnych danych - prze≈ÇƒÖczam na SearchAgent"
                        )
                        try:
                            from agents.search_agent import SearchAgent

                            search_agent = SearchAgent()
                            search_input = {"query": query, "max_results": 3}
                            search_response = await search_agent.process(search_input)
                            if search_response and search_response.text:
                                return AgentResponse(
                                    success=True,
                                    text=f"[Wynik wyszukiwania internetowego]\n{search_response.text}",
                                    data={
                                        "query": query,
                                        "used_rag": bool(rag_context),
                                        "used_internet": True,
                                        "used_search_agent": True,
                                        "rag_confidence": 0.0,
                                        "use_perplexity": use_perplexity,
                                        "use_bielik": use_bielik,
                                        "session_id": session_id,
                                        "anti_hallucination": True,
                                        "trigger": "hallucination_patterns_with_search",
                                    },
                                )
                        except Exception as e:
                            logger.error(
                                f"[GeneralConversationAgent] B≈ÇƒÖd podczas wywo≈Çania SearchAgent w hallucination handler: {e}"
                            )

                    # Wybierz odpowiedni komunikat w zale≈ºno≈õci od typu query
                    if is_recipe_query(query):
                        safe_message = "Nie mam dostƒôpnych sk≈Çadnik√≥w do tego przepisu. Sprawd≈∫ swojƒÖ spi≈ºarniƒô."
                    elif is_product_query(query):
                        safe_message = "Nie mam informacji o tym produkcie."
                    elif is_person_query(query) and not is_known_person(query):
                        safe_message = "Nie mam informacji o tej osobie."
                    elif is_event_query(query):
                        safe_message = "Nie mam aktualnych informacji o tym wydarzeniu."
                    elif is_future_event(query):
                        safe_message = (
                            "Nie mam informacji o tym wydarzeniu z przysz≈Ço≈õci."
                        )
                    else:
                        safe_message = (
                            "Nie mam zweryfikowanych informacji na ten temat."
                        )

                return AgentResponse(
                    success=True,
                    text=safe_message,
                    data={
                        "query": query,
                        "used_rag": bool(rag_context),
                        "used_internet": False,  # Nie u≈ºywamy SearchAgent w tym miejscu
                        "used_search_agent": False,
                        "rag_confidence": 0.0,
                        "use_perplexity": use_perplexity,
                        "use_bielik": use_bielik,
                        "session_id": session_id,
                        "anti_hallucination": True,
                        "trigger": "hallucination_patterns",
                    },
                )

            # --- POST-PROCESSING ANTI-HALLUCINATION FILTER ---
            # Zaawansowany filtr z fuzzy matching i detekcjƒÖ wzorc√≥w halucynacji
            def _should_switch_to_search(
                query: str, rag_context: str, response: str, rag_confidence: float
            ) -> bool:
                # Je≈õli nie ma kontekstu RAG i nie ma odpowiedzi lub odpowied≈∫ jest pusta/niepewna
                if not rag_context or rag_confidence < 0.3:
                    return True
                if not response or not response.strip():
                    return True

                # Typowe frazy braku wiedzy
                lower_resp = response.lower()
                knowledge_lack_phrases = [
                    "nie wiem",
                    "nie jestem pewien",
                    "nie posiadam informacji",
                    "nie mogƒô odpowiedzieƒá",
                    "nie znalaz≈Çem",
                    "nie znalaz≈Çam",
                    "nie jestem w stanie",
                    "nie mam wystarczajƒÖcych danych",
                    "nie potrafiƒô odpowiedzieƒá",
                    "nie mam wiedzy",
                    "nie mam informacji",
                    "nie mogƒô znale≈∫ƒá",
                    "nie mogƒô udzieliƒá odpowiedzi",
                    "nie mam dostƒôpu do aktualnych",
                    "nie mam informacji o tej osobie",
                    "nie mam aktualnych informacji",
                    "nie posiadam najnowszych danych",
                ]

                for phrase in knowledge_lack_phrases:
                    if phrase in lower_resp:
                        return True

                # Detekcja pyta≈Ñ wymagajƒÖcych aktualnych danych
                current_data_indicators = [
                    "aktualna cena",
                    "najnowsza wiadomo≈õƒá",
                    "dzisiaj",
                    "obecnie",
                    "najnowsze informacje",
                    "aktualne dane",
                    "obecna sytuacja",
                    "ostatnie wiadomo≈õci",
                    "co siƒô dzieje",
                    "naj≈õwie≈ºsze",
                ]

                query_lower = query.lower()
                for indicator in current_data_indicators:
                    if indicator in query_lower:
                        return True

                # Detekcja og√≥lnych/wymijajƒÖcych odpowiedzi (kr√≥tsze ni≈º 100 znak√≥w lub bardzo og√≥lne)
                if len(response.strip()) < 100:
                    return True

                # Detekcja specyficznych wymijajƒÖcych odpowiedzi
                evasive_patterns = [
                    "nie mam informacji o tej osobie",
                    "test response for",
                    "nie mam dostƒôpu do",
                    "niestety nie mam",
                ]

                return any(pattern in lower_resp for pattern in evasive_patterns)

            # Sprawd≈∫ czy powinni≈õmy prze≈ÇƒÖczyƒá na SearchAgent
            used_search_agent = False
            if _should_switch_to_search(query, rag_context, response, rag_confidence):
                logger.info(
                    "[GeneralConversationAgent] Brak pewnej odpowiedzi lokalnej, prze≈ÇƒÖczam na SearchAgent/web_search"
                )
                try:
                    # Wywo≈Çaj SearchAgent
                    from agents.search_agent import SearchAgent

                    search_agent = SearchAgent()
                    search_input = {"query": query, "max_results": 3}
                    search_response = await search_agent.process(search_input)
                    if search_response and search_response.text:
                        response = f"[Wynik wyszukiwania internetowego]\n{search_response.text}"
                        used_search_agent = True
                        logger.info(
                            "[GeneralConversationAgent] SearchAgent zwr√≥ci≈Ç odpowied≈∫"
                        )
                    else:
                        response = "Nie uda≈Ço siƒô znale≈∫ƒá odpowiedzi w internecie."
                        logger.warning(
                            "[GeneralConversationAgent] SearchAgent nie zwr√≥ci≈Ç odpowiedzi"
                        )
                except Exception as e:
                    logger.error(
                        f"[GeneralConversationAgent] B≈ÇƒÖd podczas wywo≈Çania SearchAgent: {e}"
                    )
                    response = "WystƒÖpi≈Ç b≈ÇƒÖd podczas wyszukiwania w internecie."

            logger.debug("GeneralConversationAgent.process completed successfully")
            return AgentResponse(
                success=True,
                text=response,
                data={
                    "query": query,
                    "used_rag": bool(rag_context),
                    "used_internet": used_search_agent or bool(internet_context),
                    "used_search_agent": used_search_agent,
                    "rag_confidence": rag_confidence,
                    "use_perplexity": use_perplexity,
                    "use_bielik": use_bielik,
                    "session_id": session_id,
                },
            )

        except Exception as e:
            logger.error(f"Error in GeneralConversationAgent: {e!s}", exc_info=True)

            # Extract query for proper error response
            query = self._extract_query_from_input(input_data)

            # Provide graceful error handling based on error type
            if "ollama" in str(e).lower() or "connection" in str(e).lower():
                error_response = "Przepraszam, obecnie mam problemy z po≈ÇƒÖczeniem do systemu AI. Spr√≥buj ponownie za chwilƒô."
            elif "timeout" in str(e).lower():
                error_response = "Operacja przekroczy≈Ça dozwolony czas. Spr√≥buj zadaƒá prostsze pytanie."
            elif "rate" in str(e).lower() or "limit" in str(e).lower():
                error_response = "Przekroczono limit zapyta≈Ñ. Poczekaj chwilƒô przed kolejnym zapytaniem."
            else:
                error_response = "Przepraszam, wystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas przetwarzania Twojego zapytania."

            return AgentResponse(
                success=False,
                text=error_response,
                error=str(e),
                data={
                    "query": query,
                    "used_rag": False,
                    "used_internet": False,
                    "used_search_agent": False,
                    "rag_confidence": 0.0,
                    "use_perplexity": input_data.get("use_perplexity", False),
                    "use_bielik": input_data.get("use_bielik", True),
                    "session_id": input_data.get("session_id", ""),
                    "error_type": type(e).__name__,
                },
            )

    def _extract_query_from_input(self, input_data: dict[str, Any]) -> str:
        """Defensively extract query from possible fields in input_data."""
        possible_fields = ["query", "task", "user_command", "command", "text"]
        for field in possible_fields:
            value = input_data.get(field)
            if value and isinstance(value, str) and value.strip():
                return value.strip()
        return ""

    @cached_async(rag_cache)
    async def _get_rag_context(self, query: str) -> tuple[str, float]:
        """Pobiera kontekst z RAG i ocenia jego pewno≈õƒá."""
        rag_start_time = time.time()
        
        try:
            # 1. Stw√≥rz wektor dla zapytania
            query_embedding_list = await _get_mmlw_client().embed_text(query)
            if not query_embedding_list:
                logger.warning("Failed to generate query embedding for RAG context.")
                duration = time.time() - rag_start_time
                performance_monitor.record_operation("rag_search", duration, {"error": "embedding_failed"})
                return "", 0.0
            query_embedding = np.array([query_embedding_list], dtype=np.float32)

            # 2. Przeszukaj bazƒô wektorowƒÖ (bez min_similarity)
            # Zwiƒôkszamy k, aby mieƒá wiƒôcej kandydat√≥w do filtrowania
            if _get_vector_store() is not None:
                search_results = await _get_vector_store().search(query_embedding, k=5)
            else:
                logger.warning("vector_store is not available")
                duration = time.time() - rag_start_time
                performance_monitor.record_operation("rag_search", duration, {"error": "vector_store_unavailable"})
                return "", 0.0

            if not search_results:
                duration = time.time() - rag_start_time
                performance_monitor.record_operation("rag_search", duration, {"no_results": True})
                return "", 0.0

            # 3. Rƒôcznie odfiltruj wyniki poni≈ºej progu podobie≈Ñstwa
            min_similarity_threshold = 0.7
            filtered_results = [
                (doc, sim)
                for doc, sim in search_results
                if sim >= min_similarity_threshold
            ]

            if not filtered_results:
                duration = time.time() - rag_start_time
                performance_monitor.record_operation("rag_search", duration, {"no_filtered_results": True})
                return "", 0.0

            # 4. Przetw√≥rz i sformatuj odfiltrowane wyniki
            avg_confidence = sum(sim for _, sim in filtered_results) / len(
                filtered_results
            )

            context_parts = []
            if filtered_results:
                doc_texts = [
                    f"- {doc.content} (≈πr√≥d≈Ço: {doc.metadata.get('filename', 'Brak nazwy')})"
                    for doc, sim in filtered_results
                    if doc.content
                ]
                if doc_texts:
                    context_parts.append("Dokumenty:\n" + "\n".join(doc_texts[:2]))

            duration = time.time() - rag_start_time
            performance_monitor.record_operation("rag_search", duration, {"confidence": avg_confidence, "results_count": len(filtered_results)})
            
            return "\n\n".join(context_parts) if context_parts else "", avg_confidence

        except Exception as e:
            duration = time.time() - rag_start_time
            performance_monitor.record_operation("rag_search", duration, {"error": str(e)})
            logger.warning(f"Error getting RAG context: {e!s}")
            return "", 0.0

    @cached_async(internet_cache)
    async def _get_internet_context(self, query: str, use_perplexity: bool) -> str:
        """Pobiera informacje z internetu z weryfikacjƒÖ wiedzy"""
        internet_start_time = time.time()
        
        try:
            # Perplexity API removed - using web_search fallback
            if _get_web_search() is not None:
                search_results = await _get_web_search().search(query, max_results=3)
                if search_results:
                    duration = time.time() - internet_start_time
                    performance_monitor.record_operation("internet_search", duration, {"results_count": len(search_results)})
                    return "Informacje z internetu:\n" + "\n".join(
                        [
                            f"**{result.get('title', 'Brak tytu≈Çu')}**\n{result.get('snippet', 'Brak opisu')}\n≈πr√≥d≈Ço: {result.get('url', 'Brak URL')}"
                            for result in search_results[:2]
                        ]
                    )
            else:
                logger.warning("web_search is not available")
                duration = time.time() - internet_start_time
                performance_monitor.record_operation("internet_search", duration, {"error": "web_search_unavailable"})
            return ""

        except Exception as e:
            duration = time.time() - internet_start_time
            performance_monitor.record_operation("internet_search", duration, {"error": str(e)})
            logger.warning(f"Error getting internet context: {e!s}")
            return ""

    async def _generate_response(
        self,
        query: str,
        rag_context: str,
        internet_context: str,
        use_perplexity: bool,
        use_bielik: bool,
    ) -> str:
        """Generuje odpowied≈∫ z wykorzystaniem wszystkich ≈∫r√≥de≈Ç.
        Informacji i weryfikacji wiedzy.
        """

        # Okre≈õl z≈Ço≈ºono≈õƒá zapytania z ulepszonƒÖ analizƒÖ
        complexity = self._determine_query_complexity_enhanced(
            query, rag_context, internet_context
        )

        # System prompt zoptymalizowany dla modelu Bielik z polskim kontekstem kulturowym
        system_prompt = self._build_bielik_optimized_prompt(complexity, rag_context, internet_context)

        # Buduj kontekst
        context_parts = []
        if rag_context:
            context_parts.append(f"KONTEKST Z DOKUMENT√ìW I BAZY DANYCH:\n{rag_context}")
        if internet_context:
            context_parts.append(f"INFORMACJE Z INTERNETU:\n{internet_context}")

        context_text = "\n\n".join(context_parts) if context_parts else ""

        # Buduj wiadomo≈õci
        messages = [{"role": "system", "content": system_prompt}]

        if context_text:
            messages.append(
                {
                    "role": "system",
                    "content": (
                        f"DOSTƒòPNE INFORMACJE:\n{context_text}\n\nKRYTYCZNE: U≈ºyj TYLKO tych "
                        "informacji do udzielenia dok≈Çadnej odpowiedzi. NIGDY nie wymy≈õlaj "
                        "dodatkowych fakt√≥w, szczeg√≥≈Ç√≥w ani informacji. Je≈õli informacji brakuje, "
                        "przyznaj to zamiast wymy≈õlaƒá. Uwzglƒôdnij informacje o weryfikacji wiedzy "
                        "je≈õli sƒÖ dostƒôpne."
                    ),
                }
            )

        messages.append({"role": "user", "content": query})

        # Generuj odpowied≈∫ u≈ºywajƒÖc odpowiedniego modelu
        try:
            logger.info(f"Enhanced complexity determination: {complexity}")
            logger.info(f"Enhanced complexity determination: {complexity}")

            # Wybierz optymalny model Bielik
            model_name = self._select_model(complexity, use_bielik)
            logger.info(f"Selected optimized Bielik model: {model_name}")

            # Wykryj intencjƒô u≈ºytkownika dla lepszego dostosowania
            user_intent = self._detect_user_intent_bielik(query)
            logger.info(f"Detected user intent: {user_intent}")
            
            # Pobierz styl odpowiedzi na podstawie intencji
            response_style = self._get_bielik_response_style(user_intent, complexity)
            
            # Optymalizuj parametry dla wybranego modelu Bielik
            bielik_params = self._get_bielik_parameters(complexity, model_name)
            
            # Dostosuj parametry na podstawie stylu odpowiedzi
            if response_style:
                bielik_params.update({
                    "max_tokens": response_style.get("max_tokens", bielik_params["max_tokens"]),
                    "temperature": response_style.get("temperature", bielik_params["temperature"])
                })
            
            logger.info(f"Using optimized Bielik parameters: {bielik_params}")

            # Zastosuj tryb konwersacyjny (domy≈õlnie przyjazny)
            messages = await self._apply_conversation_mode(messages, "friendly")
            
            # Dodaj informacjƒô o stylu odpowiedzi do system message
            if messages and messages[0]["role"] == "system" and response_style:
                messages[0]["content"] += f"\n\nSTYL ODPOWIEDZI: {response_style['approach']}"

            response = await hybrid_llm_client.chat(
                messages=messages,
                model=model_name,
                force_complexity=complexity,
                stream=False,
                **bielik_params  # Dodaj zoptymalizowane parametry
            )

            # Sprawd≈∫ czy response jest s≈Çownikiem (nie AsyncGenerator)
            if (
                isinstance(response, dict)
                and "message" in response
                and "content" in response["message"]
            ):
                response_text = response["message"]["content"]

                # Sprawd≈∫ czy odpowied≈∫ wskazuje na brak wiedzy
                if self._indicates_lack_of_knowledge(response_text):
                    logger.info(
                        f"Response indicates lack of knowledge, switching to search mode for query: {query}"
                    )
                    return await self._switch_to_search_mode(
                        query, use_perplexity, use_bielik
                    )

                return response_text
            return "Przepraszam, nie uda≈Ço siƒô wygenerowaƒá odpowiedzi."

        except Exception as e:
            logger.error(f"Error generating response: {e!s}")

            # Je≈õli to b≈ÇƒÖd LLM, spr√≥buj z fallback modelem
            if "LLM error" in str(e) or "timeout" in str(e).lower():
                logger.info("Attempting fallback to stable model")
                try:
                    # U≈ºyj stabilnego modelu Bielik 4.5B jako fallback
                    fallback_response = await hybrid_llm_client.chat(
                        messages=messages,
                        model="SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0",
                        force_complexity=ModelComplexity.SIMPLE,
                        stream=False,
                    )

                    # Sprawd≈∫ czy fallback_response jest s≈Çownikiem
                    if (
                        isinstance(fallback_response, dict)
                        and "message" in fallback_response
                        and "content" in fallback_response["message"]
                    ):
                        logger.info("Fallback model response successful")
                        response_text = fallback_response["message"]["content"]

                        # Sprawd≈∫ czy odpowied≈∫ wskazuje na brak wiedzy
                        if self._indicates_lack_of_knowledge(response_text):
                            logger.info(
                                f"Fallback response indicates lack of knowledge, switching to search mode for query: {query}"
                            )
                            return await self._switch_to_search_mode(
                                query, use_perplexity, use_bielik
                            )

                        return response_text
                except Exception as fallback_error:
                    logger.error(f"Fallback model also failed: {fallback_error!s}")

            # Ostateczny fallback - prze≈ÇƒÖcz na tryb wyszukiwania
            logger.info(
                f"All models failed, switching to search mode for query: {query}"
            )
            return await self._switch_to_search_mode(query, use_perplexity, use_bielik)

    def _indicates_lack_of_knowledge(self, response_text: str) -> bool:
        """
        Sprawdza czy odpowied≈∫ wskazuje na brak wiedzy i powinna prze≈ÇƒÖczyƒá na tryb wyszukiwania.

        Args:
            response_text: Tekst odpowiedzi do sprawdzenia

        Returns:
            bool: True je≈õli odpowied≈∫ wskazuje na brak wiedzy
        """
        response_lower = response_text.lower()

        # Wzorce wskazujƒÖce na brak wiedzy
        lack_of_knowledge_patterns = [
            "nie mam pewnych informacji",
            "nie mam informacji",
            "nie znam",
            "nie wiem",
            "nie mam danych",
            "nie mam wiedzy",
            "nie mogƒô odpowiedzieƒá",
            "nie potrafiƒô odpowiedzieƒá",
            "nie mam dostƒôpu do informacji",
            "nie mam odpowiedzi",
            "nie mam pewno≈õci",
            "nie jestem pewien",
            "nie jestem pewna",
            "nie mogƒô potwierdziƒá",
            "nie mam informacji o",
            "nie istnieje",
            "nie rozpoznajƒô",
            "nie znam takiej osoby",
            "nie znam takiego produktu",
            "nie mam danych na ten temat",
            "nie mam wiedzy na ten temat",
            "nie mogƒô udzieliƒá informacji",
            "nie potrafiƒô udzieliƒá informacji",
            "nie mam dostƒôpu do danych",
            "nie mam dostƒôpu do wiedzy",
            "nie mogƒô znale≈∫ƒá informacji",
            "nie potrafiƒô znale≈∫ƒá informacji",
            "nie mam odpowiednich informacji",
            "nie mam aktualnych informacji",
            "nie mam szczeg√≥≈Çowych informacji",
            "nie mam pe≈Çnych informacji",
            "nie mam kompletnych informacji",
            "nie mam wystarczajƒÖcych informacji",
            "nie mam dok≈Çadnych informacji",
            "nie mam precyzyjnych informacji",
            "nie mam konkretnych informacji",
            "nie mam szczeg√≥≈Çowych danych",
            "nie mam aktualnych danych",
            "nie mam odpowiednich danych",
            "nie mam kompletnych danych",
            "nie mam pe≈Çnych danych",
            "nie mam wystarczajƒÖcych danych",
            "nie mam dok≈Çadnych danych",
            "nie mam precyzyjnych danych",
            "nie mam konkretnych danych",
        ]

        # Sprawd≈∫ czy odpowied≈∫ zawiera kt√≥rykolwiek z wzorc√≥w
        for pattern in lack_of_knowledge_patterns:
            if pattern in response_lower:
                return True

        # Sprawd≈∫ czy odpowied≈∫ jest bardzo kr√≥tka (mo≈ºe wskazywaƒá na brak wiedzy)
        if len(response_text.strip()) < 50:
            # Sprawd≈∫ czy nie jest to prosty potwierdzenie lub zaprzeczenie
            simple_responses = [
                "tak",
                "nie",
                "ok",
                "dobrze",
                "rozumiem",
                "jasne",
                "zgoda",
            ]
            if response_lower.strip() not in simple_responses:
                return True

        return False

    async def _switch_to_search_mode(
        self, query: str, use_perplexity: bool, use_bielik: bool
    ) -> str:
        """
        Automatycznie prze≈ÇƒÖcza na tryb wyszukiwania gdy model nie zna odpowiedzi.

        Args:
            query: Zapytanie u≈ºytkownika
            use_perplexity: Czy u≈ºywaƒá Perplexity
            use_bielik: Czy u≈ºywaƒá modelu Bielik

        Returns:
            str: Odpowied≈∫ z trybu wyszukiwania
        """
        try:
            logger.info(f"Switching to search mode for query: {query}")

            # Utw√≥rz SearchAgent
            from agents.search_agent import SearchAgent

            search_agent = SearchAgent()

            # Przygotuj dane wej≈õciowe dla SearchAgent
            search_input = {
                "query": query,
                "model": (
                    "SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M" if use_bielik else None
                ),
                "max_results": 5,
            }

            # Wykonaj wyszukiwanie
            search_response = await search_agent.process(search_input)

            if search_response.success and search_response.text:
                # Dodaj informacjƒô o prze≈ÇƒÖczeniu na tryb wyszukiwania
                enhanced_response = (
                    f"üîç **Prze≈ÇƒÖczono na tryb wyszukiwania**\n\n"
                    f"{search_response.text}\n\n"
                    f"*Informacje zosta≈Çy znalezione w internecie.*"
                )
                return enhanced_response
            else:
                # Je≈õli wyszukiwanie nie powiod≈Ço siƒô, zwr√≥ƒá informacjƒô
                return (
                    f"Przepraszam, nie uda≈Ço mi siƒô znale≈∫ƒá odpowiednich informacji "
                    f"w internecie dla zapytania: '{query}'. "
                    f"Spr√≥buj sformu≈Çowaƒá zapytanie inaczej lub sprawd≈∫ pisowniƒô."
                )

        except Exception as e:
            logger.error(f"Error switching to search mode: {e!s}")
            return (
                "Przepraszam, wystƒÖpi≈Ç problem podczas wyszukiwania informacji. "
                "Spr√≥buj ponownie za chwilƒô."
            )

    def _determine_query_complexity(
        self, query: str, rag_context: str, internet_context: str
    ) -> ModelComplexity:
        """
        Okre≈õla z≈Ço≈ºono≈õƒá zapytania na podstawie jego tre≈õci i dostƒôpnego kontekstu.

        Args:
            query: Zapytanie u≈ºytkownika
            rag_context: Kontekst z RAG (mo≈ºe byƒá tuple (str, float) lub str)
            internet_context: Kontekst z internetu

        Returns:
            ModelComplexity: Poziom z≈Ço≈ºono≈õci zapytania
        """
        query_lower = query.lower()

        # Je≈õli zapytanie jest bardzo kr√≥tkie, to jest proste
        if len(query) < 10:
            return ModelComplexity.SIMPLE

        # Je≈õli mamy du≈ºo kontekstu, to zapytanie jest z≈Ço≈ºone
        # Obs≈Çuga przypadku gdy rag_context jest tuple (str, float)
        rag_text = rag_context[0] if isinstance(rag_context, tuple) else rag_context
        combined_context = (rag_text or "") + (internet_context or "")
        if len(combined_context) > 1000:
            return ModelComplexity.COMPLEX

        # S≈Çowa kluczowe sugerujƒÖce z≈Ço≈ºone zapytanie
        complex_keywords = [
            "por√≥wnaj",
            "przeanalizuj",
            "wyja≈õnij",
            "zinterpretuj",
            "oce≈Ñ",
            "podsumuj",
            "zreferuj",
            "przedstaw argumenty",
            "uzasadnij",
            "zaprojektuj",
            "stw√≥rz",
            "napisz",
            "wymy≈õl",
            "zaproponuj",
        ]

        if any(keyword in query_lower for keyword in complex_keywords):
            return ModelComplexity.COMPLEX

        # Domy≈õlnie u≈ºywamy standardowej z≈Ço≈ºono≈õci
        return ModelComplexity.STANDARD

    def _select_model(self, complexity: ModelComplexity, use_bielik: bool) -> str:
        """
        Wybiera optymalny model Bielik na podstawie z≈Ço≈ºono≈õci zapytania

        Args:
            complexity: Z≈Ço≈ºono≈õƒá zapytania
            use_bielik: Czy u≈ºywaƒá modelu Bielik (zawsze True dla tego agenta)

        Returns:
            Nazwa modelu do u≈ºycia - zoptymalizowana wersja Bielika
        """
        # Adaptacyjny wyb√≥r modelu Bielik na podstawie z≈Ço≈ºono≈õci
        if complexity == ModelComplexity.SIMPLE:
            # Dla prostych zapyta≈Ñ u≈ºywaj najszybszego modelu
            return "SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0"
        elif complexity == ModelComplexity.STANDARD:
            # Dla standardowych zapyta≈Ñ u≈ºywaj zbalansowanego modelu
            return "SpeakLeash/bielik-7b-v2.1-instruct:Q5_K_M"
        else:  # COMPLEX
            # Dla z≈Ço≈ºonych zapyta≈Ñ u≈ºywaj najpotƒô≈ºniejszego modelu
            return "SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M"

    async def process_stream(
        self, input_data: dict[str, Any]
    ) -> AsyncGenerator[dict[str, Any], None]:
        """Process general conversation input with streaming response"""
        try:
            query = input_data.get("query", "")
            context = input_data.get("context", [])
            use_perplexity = input_data.get("use_perplexity", False)
            use_bielik = input_data.get("use_bielik", True)

            # Determine if this is a simple query
            is_simple_query = self._is_simple_query(query)

            # For simple queries, use a smaller model
            if is_simple_query:
                force_complexity = ModelComplexity.SIMPLE
                logger.info(f"Using SIMPLE model for query: {query}")
            else:
                force_complexity = None
                logger.info(f"Using standard model selection for query: {query}")

            # Run RAG and internet search in parallel
            rag_task = asyncio.create_task(self._get_rag_results(query))
            internet_task = asyncio.create_task(self._get_internet_results(query))

            # First yield a message that we're gathering information
            yield {
                "text": "Zbieram informacje...",
                "data": {"status": "gathering_info"},
                "success": True,
            }

            # Wait for both tasks to complete
            rag_results, internet_results = await asyncio.gather(
                rag_task, internet_task
            )

            # Yield a message that we're processing the information
            yield {
                "text": "\nAnalizujƒô zebrane dane...",
                "data": {"status": "processing_info"},
                "success": True,
            }

            # Combine context from both sources
            combined_context = self._combine_context(rag_results, internet_results)

            # Format the context for the LLM
            formatted_context = self._format_context_for_llm(combined_context)

            # Prepare messages for the LLM
            messages = self._prepare_messages(query, context, formatted_context)

            # Generate streaming response using the LLM
            stream_response = await hybrid_llm_client.chat(
                messages=messages,
                stream=True,
                use_perplexity=use_perplexity,
                use_bielik=use_bielik,
                force_complexity=force_complexity,
            )

            # Clear the initial messages
            yield {
                "text": "",
                "data": {"status": "responding", "clear_previous": True},
                "success": True,
            }

            # Stream the response chunks
            full_text = ""
            async for chunk in stream_response:
                if (
                    isinstance(chunk, dict)
                    and "message" in chunk
                    and "content" in chunk["message"]
                ):
                    content = chunk["message"]["content"]
                    full_text += content
                    yield {
                        "text": content,
                        "data": {"status": "streaming"},
                        "success": True,
                    }

            # Final chunk with complete data
            yield {
                "text": "",  # No additional text
                "data": {
                    "status": "complete",
                    "context_used": combined_context[
                        :2
                    ],  # Include first 2 context items for transparency
                },
                "success": True,
            }

        except Exception as e:
            logger.error(f"Error in GeneralConversationAgent streaming: {e!s}")
            yield {
                "text": f"Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd: {e!s}",
                "data": {"status": "error"},
                "success": False,
            }

    def _is_simple_query(self, query: str) -> bool:
        """Determine if a query is simple based on length and complexity"""
        # Simple heuristic: short queries are likely simple
        if len(query) < 20:
            return True

        # Check for common simple phrases
        simple_phrases = [
            "dziƒôkujƒô",
            "dziƒôki",
            "rozumiem",
            "ok",
            "dobrze",
            "tak",
            "nie",
            "mo≈ºe",
            "cze≈õƒá",
            "hej",
            "witaj",
            "do widzenia",
            "pa",
            "≈ºegnaj",
        ]

        query_lower = query.lower()
        return any(phrase in query_lower for phrase in simple_phrases)

    def _generate_simple_response(self, query: str) -> str:
        """Generate appropriate response for simple queries like greetings"""
        query_lower = query.lower().strip()

        # Greetings
        if any(
            greeting in query_lower
            for greeting in ["cze≈õƒá", "hej", "witaj", "hi", "hello"]
        ):
            return "Cze≈õƒá! Jak mogƒô Ci pom√≥c dzisiaj? üòä"

        # Thanks
        if any(
            thanks in query_lower
            for thanks in ["dziƒôkujƒô", "dziƒôki", "thank you", "thanks"]
        ):
            return "Nie ma sprawy! üòä"

        # Agreement
        if any(agree in query_lower for agree in ["ok", "dobrze", "tak", "rozumiem"]):
            return "≈öwietnie! üòä"

        # Disagreement
        if any(disagree in query_lower for disagree in ["nie", "no"]):
            return "Rozumiem. Czy mogƒô pom√≥c w czym≈õ innym?"

        # Uncertainty
        if any(uncertain in query_lower for uncertain in ["mo≈ºe", "maybe"]):
            return "Rozumiem Twoje wƒÖtpliwo≈õci. Daj znaƒá, je≈õli bƒôdziesz potrzebowaƒá pomocy!"

        # Goodbye
        if any(
            goodbye in query_lower
            for goodbye in ["do widzenia", "pa", "≈ºegnaj", "bye", "goodbye"]
        ):
            return "Do widzenia! Mi≈Çego dnia! üëã"

        # Default for other short queries
        return "Rozumiem! Jak mogƒô Ci pom√≥c?"

    @cached_async(rag_cache)  # Cache RAG results for 1 hour
    async def _get_rag_results(self, query: str) -> list[dict[str, str]]:
        """Get results from RAG system with caching"""
        try:
            # Placeholder for actual RAG implementation
            # In a real system, this would query a vector database
            logger.info(f"Getting RAG results for: {query}")
            await asyncio.sleep(0.1)  # Simulate some processing time
            return []  # Return empty list as placeholder
        except Exception as e:
            logger.error(f"Error getting RAG results: {e!s}")
            return []

    @cached_async(internet_cache)  # Cache internet results for 30 minutes
    async def _get_internet_results(self, query: str) -> list[dict[str, str]]:
        """Get results from internet search with caching"""
        try:
            logger.info(f"Getting internet results for: {query}")
            # U≈ºyj web_search jako obiektu, nie funkcji
            if _get_web_search() is not None:
                results = await _get_web_search().search(query, max_results=3)
                return results if results else []
            else:
                logger.warning("web_search is not available")
                return []
        except Exception as e:
            logger.error(f"Error getting internet results: {e!s}")
            return []

    def _combine_context(
        self, rag_results: list[dict[str, str]], internet_results: list[dict[str, str]]
    ) -> list[dict[str, str]]:
        """Combine context from RAG and internet search"""
        # Simple combination strategy: RAG results first, then internet
        combined = []

        # Add RAG results if available
        if rag_results:
            combined.extend(rag_results)

        # Add internet results if available
        if internet_results:
            combined.extend(internet_results)

        return combined

    def _format_context_for_llm(self, context: list[dict[str, str]]) -> str:
        """Format context for LLM input"""
        if not context:
            return ""

        formatted = "Oto informacje, kt√≥re mogƒÖ byƒá pomocne:\n\n"

        for i, item in enumerate(context, 1):
            title = item.get("title", f"≈πr√≥d≈Ço {i}")
            content = item.get("content", "")
            url = item.get("url", "")

            formatted += f"--- {title} ---\n"
            formatted += f"{content}\n"
            if url:
                formatted += f"≈πr√≥d≈Ço: {url}\n"
            formatted += "\n"

        return formatted

    def _prepare_messages(
        self, query: str, conversation_history: list[dict[str, str]], context: str
    ) -> list[dict[str, str]]:
        """Prepare messages for LLM with optimized context
        and conversation history."""
        messages = []

        # Zoptymalizowany system message dla modelu Bielik
        system_message = (
            "Jeste≈õ inteligentnym polskim asystentem AI opartym na modelu Bielik. "
            "Specjalizujesz siƒô w naturalnych konwersacjach w jƒôzyku polskim. "
            "Odpowiadaj pomocnie, precyzyjnie i naturalnie. "
            "Dostosowuj d≈Çugo≈õƒá odpowiedzi do potrzeb - od zwiƒôz≈Çych po szczeg√≥≈Çowe wyja≈õnienia. "
            "Wykorzystuj polskƒÖ kulturƒô i kontekst lokalny w swoich odpowiedziach."
        )

        if context:
            system_message += "\n\nKontekst:\n" + context

        messages.append({"role": "system", "content": system_message})

        # Dodaj historiƒô konwersacji (ostatnie 3 wiadomo≈õci)
        if conversation_history:
            recent_history = conversation_history[-3:]
            for msg in recent_history:
                if msg.get("role") in ["user", "assistant"]:
                    messages.append(msg)

        # Dodaj aktualne zapytanie
        messages.append({"role": "user", "content": query})

        return messages

    def _is_date_query(self, query: str) -> bool:
        """Sprawdza czy zapytanie dotyczy daty/czasu"""
        query_lower = query.lower()

        # Wyklucz zapytania o pogodƒô
        weather_keywords = [
            "weather",
            "pogoda",
            "temperature",
            "temperatura",
            "rain",
            "deszcz",
            "snow",
            "≈õnieg",
        ]
        if any(keyword in query_lower for keyword in weather_keywords):
            return False

        # Wyklucz zapytania o inne tematy, kt√≥re mogƒÖ zawieraƒá s≈Çowa zwiƒÖzane z czasem
        exclude_keywords = [
            "weather",
            "pogoda",
            "temperature",
            "temperatura",
            "forecast",
            "prognoza",
        ]
        if any(keyword in query_lower for keyword in exclude_keywords):
            return False

        # Specyficzne wzorce dla zapyta≈Ñ o datƒô
        date_patterns = [
            r"\b(jaki|which|what)\s+(dzisiaj|today|dzie≈Ñ|day)\b",
            r"\b(kiedy|when)\s+(jest|is)\b",
            r"\b(dzisiaj|today)\s+(jest|is)\b",
            r"\b(podaj|tell)\s+(mi|me|dzisiejszƒÖ|today's)?\s*(datƒô|date)\b",
            r"\b(jaki|what)\s+(to|is)\s+(dzie≈Ñ|day)\b",
            r"\b(dzie≈Ñ|day)\s+(tygodnia|of\s+week)\b",
            r"\b(data|date)\s+(dzisiaj|today)\b",
            r"\b(dzisiaj|today)\s+(data|date)\b",
            r"\b(jaki|what)\s+(mamy|do\s+we\s+have)\s+(dzisiaj|today)\b",
            r"\b(kt√≥ry|which)\s+(dzie≈Ñ|day)\s+(dzisiaj|today)\b",
            r"\b(podaj|tell)\s+(dzisiejszƒÖ|today's)\s+(datƒô|date)\b",
            r"\b(dzisiejsza|today's)\s+(data|date)\b",
        ]

        import re

        for pattern in date_patterns:
            if re.search(pattern, query_lower):
                return True

        # Dodatkowe s≈Çowa kluczowe tylko je≈õli nie ma kontekstu pogodowego
        date_keywords = [
            "dzisiaj",
            "dzi≈õ",
            "today",
            "wczoraj",
            "yesterday",
            "jutro",
            "tomorrow",
            "dzie≈Ñ",
            "day",
            "miesiƒÖc",
            "month",
            "rok",
            "year",
            "godzina",
            "hour",
            "czas",
            "time",
            "kiedy",
            "when",
            "kt√≥ry dzie≈Ñ",
            "what day",
            "jaki dzie≈Ñ",
            "which day",
            "jaki dzisiaj",
            "what today",
            "kt√≥ry to dzie≈Ñ",
            "what day is it",
            "jaki dzisiaj dzie≈Ñ",
            "what day is today",
            "poniedzia≈Çek",
            "monday",
            "wtorek",
            "tuesday",
            "≈õroda",
            "wednesday",
            "czwartek",
            "thursday",
            "piƒÖtek",
            "friday",
            "sobota",
            "saturday",
            "niedziela",
            "sunday",
        ]

        # Sprawd≈∫ czy zapytanie zawiera g≈Ç√≥wnie s≈Çowa zwiƒÖzane z datƒÖ
        date_word_count = sum(1 for keyword in date_keywords if keyword in query_lower)
        total_words = len(query_lower.split())

        # Je≈õli wiƒôcej ni≈º 50% s≈Ç√≥w to s≈Çowa zwiƒÖzane z datƒÖ,
        # to prawdopodobnie zapytanie o datƒô
        return bool(date_word_count > 0 and date_word_count / total_words > 0.3)

    def _is_pantry_query(self, query: str) -> bool:
        """Check if query is about pantry/inventory"""
        pantry_keywords = [
            "spi≈ºarnia",
            "lod√≥wka",
            "magazyn",
            "produkty",
            "jedzenie",
            "≈ºywno≈õƒá",
            "co mam",
            "co jest",
            "sprawd≈∫",
            "lista",
            "inwentarz",
            "zapasy",
            "sk≈Çadniki",
            "przepis",
            "gotowanie",
            "kuchnia",
            "jedzenie",
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in pantry_keywords)

    def _should_use_pantry_tools(self, query: str) -> bool:
        """Determine if pantry tools should be used"""
        return self._is_pantry_query(query)

    async def _execute_pantry_tools(self, query: str, db: AsyncSession) -> str:
        """Execute pantry-related tools"""
        try:
            from agents.tools.tools import get_pantry_summary

            # Get pantry summary
            summary = await get_pantry_summary(db)

            if "co mam" in query.lower() or "co jest" in query.lower():
                # User wants to know what's in pantry
                result = "W Twojej spi≈ºarni masz:\n\n"
                result += f"‚Ä¢ ≈ÅƒÖcznie produkt√≥w: {summary['total_items']}\n"
                result += f"‚Ä¢ W magazynie: {summary['in_stock']}\n"
                result += f"‚Ä¢ Niski stan: {summary['low_stock']}\n"
                result += f"‚Ä¢ Brak: {summary['out_of_stock']}\n"

                if summary["expiring_soon"] > 0:
                    result += f"‚Ä¢ Wkr√≥tce przeterminowane: {summary['expiring_soon']}\n"

                if summary["categories"]:
                    result += "\nProdukty wed≈Çug kategorii:\n"
                    for category, items in summary["categories"].items():
                        result += f"‚Ä¢ {category}: {len(items)} produkt√≥w\n"

                return result
            else:
                # General pantry info
                return f"Podsumowanie spi≈ºarni: {summary['total_items']} produkt√≥w, {summary['in_stock']} w magazynie, {summary['low_stock']} z niskim stanem."

        except Exception as e:
            logger.error(f"Error executing pantry tools: {e}")
            return f"Przepraszam, nie uda≈Ço siƒô pobraƒá informacji o spi≈ºarni: {e!s}"

    def _get_available_tools(self) -> list[str]:
        """Get list of available tools for the agent"""
        return [
            "search_web",
            "get_weather",
            "convert_units",
            "get_current_time",
            "calculate",
            "get_pantry_info",
            "check_pantry_for_ingredients",
        ]

    def _is_weather_query(self, query: str) -> bool:
        """Check if query is weather-related"""
        weather_keywords = {
            "pogoda",
            "weather",
            "temperatura",
            "temperature",
            "deszcz",
            "rain",
            "≈õnieg",
            "snow",
            "wiatr",
            "wind",
            "wilgotno≈õƒá",
            "humidity",
            "s≈Ço≈Ñce",
            "sun",
            "chmury",
            "clouds",
            "burza",
            "storm",
            "mg≈Ça",
            "fog",
            "grad",
            "hail",
        }
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in weather_keywords)

    def _is_greeting(self, query: str) -> bool:
        """Check if query is a greeting"""
        greetings = {
            "cze≈õƒá",
            "czesc",
            "hej",
            "witaj",
            "dzie≈Ñ dobry",
            "dzien dobry",
            "dobry wiecz√≥r",
            "dobry wieczor",
            "hello",
            "hi",
            "good morning",
            "good evening",
            "witam",
            "siema",
        }
        query_lower = query.lower().strip()
        return any(greeting in query_lower for greeting in greetings)

    def _build_bielik_optimized_prompt(
        self, complexity: ModelComplexity, rag_context: str, internet_context: str
    ) -> str:
        """
        Buduje zoptymalizowany system prompt dla modelu Bielik
        
        Args:
            complexity: Z≈Ço≈ºono≈õƒá zapytania
            rag_context: Kontekst z RAG
            internet_context: Kontekst z internetu
            
        Returns:
            Zoptymalizowany system prompt dla Bielika
        """
        # Bazowy prompt dla modelu Bielik z polskim kontekstem
        base_prompt = (
            "Jeste≈õ Bielik - zaawansowany polski asystent AI stworzony przez SpeakLeash. "
            "Specjalizujesz siƒô w prowadzeniu naturalnych konwersacji w jƒôzyku polskim, "
            "uwzglƒôdniajƒÖc polski kontekst kulturowy, spo≈Çeczny i jƒôzykowy.\n\n"
        )
        
        # Dostosuj prompt do z≈Ço≈ºono≈õci zapytania
        if complexity == ModelComplexity.SIMPLE:
            complexity_prompt = (
                "TRYB SZYBKIEJ ODPOWIEDZI:\n"
                "- Odpowiadaj zwiƒô≈∫le i na temat\n"
                "- Maksymalnie 1-2 zdania dla prostych pyta≈Ñ\n"
                "- U≈ºywaj naturalnego, przyjaznego tonu\n"
                "- Priorytetowo traktuj jasno≈õƒá przekazu\n\n"
            )
        elif complexity == ModelComplexity.STANDARD:
            complexity_prompt = (
                "TRYB STANDARDOWEJ KONWERSACJI:\n"
                "- Udzielaj pe≈Çnych, ale zwiƒôz≈Çych odpowiedzi\n"
                "- 2-4 zdania z wyja≈õnieniami\n"
                "- Dodawaj przydatny kontekst gdy potrzebny\n"
                "- Zachowuj r√≥wnowagƒô miƒôdzy szczeg√≥≈Çowo≈õciƒÖ a jasno≈õciƒÖ\n\n"
            )
        else:  # COMPLEX
            complexity_prompt = (
                "TRYB G≈ÅƒòBOKIEJ ANALIZY:\n"
                "- Udzielaj szczeg√≥≈Çowych, wieloaspektowych odpowiedzi\n"
                "- Analizuj zagadnienia z r√≥≈ºnych perspektyw\n"
                "- U≈ºywaj strukturyzowanego formatowania gdy pomocne\n"
                "- Dodawaj przyk≈Çady i kontekst kulturowy\n"
                "- Nie ograniczaj siƒô d≈Çugo≈õciƒÖ - priorytetem jest kompletno≈õƒá\n\n"
            )
        
        # Instrukcje anty-halucynacyjne
        anti_hallucination = (
            "ZASADY WIARYGODNO≈öCI:\n"
            "- NIGDY nie wymy≈õlaj fakt√≥w, dat, nazwisk ani szczeg√≥≈Ç√≥w\n"
            "- Je≈õli nie znasz odpowiedzi, powiedz to wprost\n"
            "- Oznaczaj niepewne informacje jako 'prawdopodobnie' lub 'mo≈ºe'\n"
            "- U≈ºywaj tylko zweryfikowanych informacji z dostƒôpnych ≈∫r√≥de≈Ç\n"
            "- Dla nieznanych os√≥b/produkt√≥w odpowiadaj: 'Nie mam informacji o...'\n\n"
        )
        
        # Instrukcje kontekstowe
        context_instructions = ""
        if rag_context or internet_context:
            context_instructions = (
                "WYKORZYSTANIE KONTEKSTU:\n"
                "- Priorytetowo u≈ºywaj informacji z dostarczonych ≈∫r√≥de≈Ç\n"
                "- Zawsze wskazuj ≈∫r√≥d≈Ço informacji gdy to mo≈ºliwe\n"
                "- ≈ÅƒÖcz wiedzƒô z r√≥≈ºnych ≈∫r√≥de≈Ç w sp√≥jnƒÖ ca≈Ço≈õƒá\n"
                "- Oceniaj wiarygodno≈õƒá informacji przed u≈ºyciem\n\n"
            )
        
        # Specjalizacja polska
        polish_context = (
            "POLSKI KONTEKST:\n"
            "- U≈ºywaj polskiej terminologii i zwrot√≥w\n"
            "- Uwzglƒôdniaj polskie realia kulturowe i spo≈Çeczne\n"
            "- Rozpoznawaj polskie nazwiska, miejsca i instytucje\n"
            "- Zapamiƒôtuj informacje o u≈ºytkowniku z konwersacji\n"
            "- Je≈õli u≈ºytkownik przedstawi siƒô imieniem, u≈ºywaj go w dalszych odpowiedziach\n"
            "- Dostosowuj odpowiedzi do polskiego odbiorcy\n"
            "- U≈ºywaj polskich przyk≈Çad√≥w i analogii\n\n"
        )
        
        # Finalne instrukcje
        final_instructions = (
            "Odpowiadaj zawsze w jƒôzyku polskim, chyba ≈ºe u≈ºytkownik wyra≈∫nie prosi o inny jƒôzyk. "
            "BƒÖd≈∫ pomocny, dok≈Çadny i naturalny w komunikacji."
        )
        
        return (
            base_prompt + 
            complexity_prompt + 
            anti_hallucination + 
            context_instructions + 
            polish_context + 
            final_instructions
        )

    def _determine_query_complexity_enhanced(
        self, query: str, rag_context: str, internet_context: str
    ) -> ModelComplexity:
        """
        Ulepszona metoda okre≈õlania z≈Ço≈ºono≈õci zapytania dla modelu Bielik
        
        Args:
            query: Zapytanie u≈ºytkownika
            rag_context: Kontekst z RAG
            internet_context: Kontekst z internetu
            
        Returns:
            ModelComplexity: Poziom z≈Ço≈ºono≈õci zapytania
        """
        query_lower = query.lower()
        query_length = len(query)
        word_count = len(query.split())
        
        # Bardzo proste zapytania (pozdrowienia, kr√≥tkie odpowiedzi)
        simple_patterns = [
            "cze≈õƒá", "hej", "witaj", "dzie≈Ñ dobry", "tak", "nie", "ok", 
            "dziƒôkujƒô", "dziƒôki", "do widzenia", "pa", "pomocy", "help"
        ]
        if any(pattern in query_lower for pattern in simple_patterns) and word_count <= 3:
            return ModelComplexity.SIMPLE
        
        # Bardzo kr√≥tkie zapytania
        if query_length < 15 or word_count <= 2:
            return ModelComplexity.SIMPLE
            
        # Zapytania o fakty, definicje - standardowe
        standard_keywords = [
            "co to", "kim jest", "gdzie", "kiedy", "ile", "jaki", "jaka", 
            "jakie", "czym", "dlaczego", "definicja", "znaczenie"
        ]
        if any(keyword in query_lower for keyword in standard_keywords) and word_count <= 10:
            return ModelComplexity.STANDARD
            
        # Z≈Ço≈ºone zapytania analityczne
        complex_keywords = [
            "por√≥wnaj", "przeanalizuj", "wyja≈õnij szczeg√≥≈Çowo", "oce≈Ñ", 
            "przedstaw argumenty", "uzasadnij", "stw√≥rz", "napisz",
            "zaprojektuj", "zaproponuj rozwiƒÖzanie", "strategia", 
            "plan dzia≈Çania", "analiza", "interpretacja"
        ]
        if any(keyword in query_lower for keyword in complex_keywords):
            return ModelComplexity.COMPLEX
            
        # D≈Çugie zapytania z wieloma pytaniami
        if query_length > 200 or word_count > 30:
            return ModelComplexity.COMPLEX
            
        # Zapytania z du≈ºym kontekstem
        rag_text = rag_context[0] if isinstance(rag_context, tuple) else rag_context
        combined_context = (rag_text or "") + (internet_context or "")
        if len(combined_context) > 1500:
            return ModelComplexity.COMPLEX
            
        # Domy≈õlnie standardowy poziom
        return ModelComplexity.STANDARD
    
    def _get_bielik_parameters(self, complexity: ModelComplexity, model_name: str) -> dict:
        """
        Zwraca zoptymalizowane parametry dla modelu Bielik
        
        Args:
            complexity: Z≈Ço≈ºono≈õƒá zapytania
            model_name: Nazwa modelu Bielik
            
        Returns:
            Dict z parametrami dla modelu
        """
        base_params = {
            "temperature": 0.7,  # Balans miƒôdzy kreatywno≈õciƒÖ a precyzjƒÖ
            "top_p": 0.9,
            "max_tokens": 2048,
            "stop": None,
        }
        
        # Dostosuj parametry do z≈Ço≈ºono≈õci zapytania
        if complexity == ModelComplexity.SIMPLE:
            # Dla prostych zapyta≈Ñ - szybko i precyzyjnie
            base_params.update({
                "temperature": 0.3,  # Mniej kreatywno≈õci, wiƒôcej precyzji
                "top_p": 0.7,        # Bardziej konserwatywne wybory
                "max_tokens": 150,   # Kr√≥tsze odpowiedzi
            })
        elif complexity == ModelComplexity.COMPLEX:
            # Dla z≈Ço≈ºonych zapyta≈Ñ - wiƒôcej kreatywno≈õci i przestrzeni
            base_params.update({
                "temperature": 0.8,  # Wiƒôcej kreatywno≈õci
                "top_p": 0.95,       # Wiƒôcej r√≥≈ºnorodno≈õci
                "max_tokens": 4096,  # D≈Çu≈ºsze odpowiedzi
            })
            
        # Specjalne dostosowania dla r√≥≈ºnych wariant√≥w Bielika
        if "4.5b" in model_name:
            # Mniejszy model - bardziej konserwatywne parametry
            base_params["temperature"] = min(base_params["temperature"], 0.6)
        elif "11b" in model_name:
            # Najwiƒôkszy model - mo≈ºe pozwoliƒá sobie na wiƒôcej kreatywno≈õci
            if complexity == ModelComplexity.COMPLEX:
                base_params["temperature"] = 0.9
                
        return base_params
    
    def _get_bielik_conversation_modes(self) -> dict[str, dict]:
        """
        Zwraca dostƒôpne tryby konwersacyjne dla modelu Bielik
        
        Returns:
            Dict z r√≥≈ºnymi trybami konwersacyjnymi
        """
        return {
            "friendly": {
                "name": "Przyjazny",
                "description": "Ciep≈Çy, pomocny ton z u≈ºywaniem emotikon√≥w",
                "temperature": 0.8,
                "system_suffix": "BƒÖd≈∫ ciep≈Çy, przyjazny i u≈ºywaj emotikon√≥w gdy to naturalne. üòä"
            },
            "professional": {
                "name": "Profesjonalny", 
                "description": "Formalny, rzeczowy ton biznesowy",
                "temperature": 0.5,
                "system_suffix": "U≈ºywaj profesjonalnego, formalnego jƒôzyka. Unikaj emotikon√≥w."
            },
            "creative": {
                "name": "Kreatywny",
                "description": "Bardziej kreatywny i nietypowy spos√≥b wyra≈ºania",
                "temperature": 0.9,
                "system_suffix": "BƒÖd≈∫ kreatywny w odpowiedziach. U≈ºywaj metafor, analogii i ciekawych por√≥wna≈Ñ."
            },
            "analytical": {
                "name": "Analityczny",
                "description": "Szczeg√≥≈Çowe analizy z przyk≈Çadami i uzasadnieniami",
                "temperature": 0.6,
                "system_suffix": "Analizuj zagadnienia szczeg√≥≈Çowo. Podawaj przyk≈Çady i uzasadnienia."
            },
            "concise": {
                "name": "Zwiƒôz≈Çy",
                "description": "Kr√≥tkie, na temat odpowiedzi bez zbƒôdnych s≈Ç√≥w",
                "temperature": 0.4,
                "system_suffix": "Odpowiadaj zwiƒô≈∫le i na temat. Maksymalnie 1-2 zdania."
            },
            "educational": {
                "name": "Edukacyjny",
                "description": "Wyja≈õnia pojƒôcia krok po kroku z przyk≈Çadami",
                "temperature": 0.7,
                "system_suffix": "Wyja≈õniaj pojƒôcia krok po kroku. U≈ºywaj przyk≈Çad√≥w i prostego jƒôzyka."
            }
        }
    
    async def _apply_conversation_mode(self, messages: list, mode: str = "friendly") -> list:
        """
        Aplikuje wybrany tryb konwersacyjny do wiadomo≈õci
        
        Args:
            messages: Lista wiadomo≈õci do modelu
            mode: Wybrane tryb konwersacyjny
            
        Returns:
            Lista wiadomo≈õci z zastosowanym trybem
        """
        modes = self._get_bielik_conversation_modes()
        
        if mode not in modes:
            mode = "friendly"  # Domy≈õlny tryb
            
        mode_config = modes[mode]
        
        # Dodaj sufiks trybu do system message
        if messages and messages[0]["role"] == "system":
            messages[0]["content"] += f"\n\nTRYB KONWERSACYJNY: {mode_config['system_suffix']}"
        
        return messages
    
    def _detect_user_intent_bielik(self, query: str) -> str:
        """
        Wykrywa intencjƒô u≈ºytkownika dla lepszego dostosowania odpowiedzi Bielika
        
        Args:
            query: Zapytanie u≈ºytkownika
            
        Returns:
            Wykryta intencja u≈ºytkownika
        """
        query_lower = query.lower()
        
        # Wykrywanie r√≥≈ºnych intencji
        intents = {
            "question": ["co", "jak", "dlaczego", "kiedy", "gdzie", "kto", "ile", "jaki", "?"],
            "request": ["pom√≥≈º", "zr√≥b", "stw√≥rz", "napisz", "znajd≈∫", "sprawd≈∫", "proszƒô"],
            "conversation": ["cze≈õƒá", "dzie≈Ñ dobry", "co s≈Çychaƒá", "jak sprawy", "opowiedz"],
            "explanation": ["wyja≈õnij", "opisz", "przedstaw", "obja≈õnij", "wyt≈Çumacz"],
            "comparison": ["por√≥wnaj", "r√≥≈ºnica", "podobie≈Ñstwo", "lepszy", "gorszy"],
            "advice": ["co sƒÖdzisz", "co radzisz", "polecasz", "sugeruj", "rada"],
            "creative": ["wymy≈õl", "stw√≥rz", "zaproponuj", "wyobra≈∫ sobie", "kreatywnie"]
        }
        
        # Zlicz trafienia dla ka≈ºdej intencji
        intent_scores = {}
        for intent, keywords in intents.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                intent_scores[intent] = score
        
        # Zwr√≥ƒá intencjƒô z najwy≈ºszym wynikiem
        if intent_scores:
            return max(intent_scores.items(), key=lambda x: x[1])[0]
        
        return "general"  # Domy≈õlna intencja
    
    def _get_bielik_response_style(self, intent: str, complexity: ModelComplexity) -> dict:
        """
        Zwraca styl odpowiedzi dostosowany do intencji i z≈Ço≈ºono≈õci dla Bielika
        
        Args:
            intent: Wykryta intencja u≈ºytkownika
            complexity: Z≈Ço≈ºono≈õƒá zapytania
            
        Returns:
            Dict z ustawieniami stylu odpowiedzi
        """
        styles = {
            "question": {
                "approach": "Odpowiedz na pytanie bezpo≈õrednio i jasno",
                "max_tokens": 300 if complexity == ModelComplexity.SIMPLE else 800,
                "temperature": 0.5
            },
            "request": {
                "approach": "Wykonaj pro≈õbƒô krok po kroku",
                "max_tokens": 500 if complexity == ModelComplexity.SIMPLE else 1200,
                "temperature": 0.6
            },
            "conversation": {
                "approach": "Prowad≈∫ naturalnƒÖ, przyjaznƒÖ konwersacjƒô",
                "max_tokens": 150,
                "temperature": 0.8
            },
            "explanation": {
                "approach": "Wyja≈õnij szczeg√≥≈Çowo i zrozumiale",
                "max_tokens": 600 if complexity == ModelComplexity.STANDARD else 1500,
                "temperature": 0.6
            },
            "comparison": {
                "approach": "Por√≥wnaj systematycznie z przyk≈Çadami",
                "max_tokens": 800,
                "temperature": 0.7
            },
            "advice": {
                "approach": "Udziel przemy≈õlanej rady z uzasadnieniem",
                "max_tokens": 400,
                "temperature": 0.7
            },
            "creative": {
                "approach": "BƒÖd≈∫ kreatywny i oryginalny",
                "max_tokens": 1000,
                "temperature": 0.9
            }
        }
        
        return styles.get(intent, styles["question"])  # Domy≈õlny styl
