"""
Response Generation Module

Modu≈Ç do generowania odpowiedzi dla GeneralConversationAgent.
"""

import logging

from core.hybrid_llm_client import ModelComplexity, hybrid_llm_client

logger = logging.getLogger(__name__)


class ResponseGenerator:
    """Generator odpowiedzi z wykorzystaniem LLM"""

    @staticmethod
    async def generate_response(
        query: str,
        rag_context: str,
        internet_context: str,
        use_perplexity: bool,
        use_bielik: bool,
    ) -> str:
        """Generuje odpowied≈∫ z wykorzystaniem wszystkich ≈∫r√≥de≈Ç.
        Informacji i weryfikacji wiedzy.
        """

        # Buduj system prompt z uwzglƒôdnieniem weryfikacji wiedzy
        # i zapobiegania hallucinacjom
        system_prompt = (
            "Jeste≈õ pomocnym asystentem AI prowadzƒÖcym swobodne konwersacje. "
            "Twoim zadaniem jest udzielanie dok≈Çadnych, pomocnych i aktualnych odpowiedzi na pytania u≈ºytkownika.\n\n"
            "KRYTYCZNE ZASADY PRZECIWKO HALLUCINACJOM:\n"
            "- NIGDY nie wymy≈õlaj fakt√≥w, dat, liczb, nazw, miejsc ani szczeg√≥≈Ç√≥w\n"
            "- NIGDY nie tw√≥rz fikcyjnych przyk≈Çad√≥w, historii ani informacji\n"
            "- NIGDY nie podawaj niepewnych informacji jako fakt√≥w\n"
            "- Je≈õli nie znasz odpowiedzi, powiedz: 'Nie mam pewnych informacji na ten temat'\n"
            "- Je≈õli informacje sƒÖ niepewne, oznacz je jako 'nie jestem pewien' lub 'mo≈ºe'\n"
            "- U≈ºywaj TYLKO informacji z podanych ≈∫r√≥de≈Ç lub swojej sprawdzonej wiedzy og√≥lnej\n"
            "- Gdy brakuje informacji, przyznaj to zamiast wymy≈õlaƒá\n"
            "- Nie tw√≥rz fikcyjnych ≈∫r√≥de≈Ç, cytat√≥w ani referencji\n\n"
            "JE≈öLI U≈ªYTKOWNIK PYTA O OSOBƒò LUB PRODUKT, KT√ìREGO NIE ROZPOZNAJESZ (np. 'Jan Kowalski', 'Samsung Galaxy XYZ 2025'), ZAWSZE ODPOWIEDZ:\n"
            "- 'Nie mam informacji o osobie Jan Kowalski.'\n"
            "- 'Nie istnieje produkt Samsung Galaxy XYZ 2025.'\n"
            "NAWET JE≈öLI NAZWA BRZMI PRAWDOPODOBNIE, NIE WYMY≈öLAJ BIOGRAFII ANI SPECYFIKACJI.\n\n"
            "Przyk≈Çady:\n"
            "- Pytanie: 'Opowiedz o Janie Kowalskim, polskim naukowcu z XIX wieku'. Odpowied≈∫: 'Nie mam informacji o osobie Jan Kowalski.'\n"
            "- Pytanie: 'Jakie sƒÖ specyfikacje telefonu Samsung Galaxy XYZ 2025?'. Odpowied≈∫: 'Nie istnieje produkt Samsung Galaxy XYZ 2025.'\n\n"
            "Wykorzystuj dostƒôpne ≈∫r√≥d≈Ça informacji w kolejno≈õci priorytetu:\n"
            "1. Informacje z dokument√≥w (je≈õli dostƒôpne)\n"
            "2. Dane z bazy (je≈õli dostƒôpne)\n"
            "3. Informacje z internetu (je≈õli dostƒôpne)\n"
            "4. SprawdzonƒÖ wiedzƒô og√≥lnƒÖ (tylko fakty)\n\n"
            "Weryfikacja wiedzy:\n"
            "- Je≈õli informacje zawierajƒÖ wska≈∫niki wiarygodno≈õci, uwzglƒôdnij je w odpowiedzi\n"
            "- Oznacz informacje jako zweryfikowane (‚úÖ) lub niezweryfikowane (‚ö†Ô∏è)\n"
            "- Je≈õli wska≈∫nik wiarygodno≈õci jest niski (< 0.4), zalecaj ostro≈ºno≈õƒá\n"
            "- Zawsze podawaj ≈∫r√≥d≈Ça informacji gdy to mo≈ºliwe\n"
            "- Odr√≥≈ºniaj fakty od opinii\n\n"
            "Odpowiadaj w jƒôzyku polskim, chyba ≈ºe u≈ºytkownik prosi o innƒÖ wersjƒô jƒôzykowƒÖ."
        )

        # Buduj kontekst
        context_parts = []
        if rag_context:
            context_parts.append(f"KONTEKST Z DOKUMENT√ìW I BAZY DANYCH:\n{rag_context}")
        if internet_context:
            context_parts.append(f"INFORMACJE Z INTERNETU:\n{internet_context}")

        context_text = "\n\n".join(context_parts) if context_parts else ""

        # Buduj wiadomo≈õci
        messages = [{"role": "system", "content": system_prompt}]

        if context_text:
            messages.append(
                {
                    "role": "system",
                    "content": (
                        f"DOSTƒòPNE INFORMACJE:\n{context_text}\n\nKRYTYCZNE: U≈ºyj TYLKO tych "
                        "informacji do udzielenia dok≈Çadnej odpowiedzi. NIGDY nie wymy≈õlaj "
                        "dodatkowych fakt√≥w, szczeg√≥≈Ç√≥w ani informacji. Je≈õli informacji brakuje, "
                        "przyznaj to zamiast wymy≈õlaƒá. Uwzglƒôdnij informacje o weryfikacji wiedzy "
                        "je≈õli sƒÖ dostƒôpne."
                    ),
                }
            )

        messages.append({"role": "user", "content": query})

        # Generuj odpowied≈∫ u≈ºywajƒÖc odpowiedniego modelu
        try:
            # Okre≈õl z≈Ço≈ºono≈õƒá zapytania
            complexity = ResponseGenerator._determine_query_complexity(
                query, rag_context, internet_context
            )
            logger.info(f"Determined query complexity: {complexity}")

            # Wybierz model na podstawie z≈Ço≈ºono≈õci i flagi use_bielik
            model_name = ResponseGenerator._select_model(complexity, use_bielik)
            logger.info(f"Selected model: {model_name}")

            response = await hybrid_llm_client.chat(
                messages=messages,
                model=model_name,
                force_complexity=complexity,
                stream=False,
            )

            # Sprawd≈∫ czy response jest s≈Çownikiem (nie AsyncGenerator)
            if (
                isinstance(response, dict)
                and "message" in response
                and "content" in response["message"]
            ):
                response_text = response["message"]["content"]

                # Sprawd≈∫ czy odpowied≈∫ wskazuje na brak wiedzy
                if ResponseGenerator._indicates_lack_of_knowledge(response_text):
                    logger.info(
                        f"Response indicates lack of knowledge, switching to search mode for query: {query}"
                    )
                    return await ResponseGenerator._switch_to_search_mode(
                        query, use_perplexity, use_bielik
                    )

                return response_text
            return "Przepraszam, nie uda≈Ço siƒô wygenerowaƒá odpowiedzi."

        except Exception as e:
            logger.error(f"Error generating response: {e!s}")

            # Je≈õli to b≈ÇƒÖd LLM, spr√≥buj z fallback modelem
            if "LLM error" in str(e) or "timeout" in str(e).lower():
                logger.info("Attempting fallback to stable model")
                try:
                    # U≈ºyj stabilnego modelu Bielik 4.5B jako fallback
                    fallback_response = await hybrid_llm_client.chat(
                        messages=messages,
                        model="SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0",
                        force_complexity=ModelComplexity.SIMPLE,
                        stream=False,
                    )

                    # Sprawd≈∫ czy fallback_response jest s≈Çownikiem
                    if (
                        isinstance(fallback_response, dict)
                        and "message" in fallback_response
                        and "content" in fallback_response["message"]
                    ):
                        logger.info("Fallback model response successful")
                        response_text = fallback_response["message"]["content"]

                        # Sprawd≈∫ czy odpowied≈∫ wskazuje na brak wiedzy
                        if ResponseGenerator._indicates_lack_of_knowledge(response_text):
                            logger.info(
                                f"Fallback response indicates lack of knowledge, switching to search mode for query: {query}"
                            )
                            return await ResponseGenerator._switch_to_search_mode(
                                query, use_perplexity, use_bielik
                            )

                        return response_text
                except Exception as fallback_error:
                    logger.error(f"Fallback model also failed: {fallback_error!s}")

            # Ostateczny fallback - prze≈ÇƒÖcz na tryb wyszukiwania
            logger.info(
                f"All models failed, switching to search mode for query: {query}"
            )
            return await ResponseGenerator._switch_to_search_mode(query, use_perplexity, use_bielik)

    @staticmethod
    def _indicates_lack_of_knowledge(response_text: str) -> bool:
        """
        Sprawdza czy odpowied≈∫ wskazuje na brak wiedzy i powinna prze≈ÇƒÖczyƒá na tryb wyszukiwania.
        """
        response_lower = response_text.lower()

        # Wzorce wskazujƒÖce na brak wiedzy
        lack_of_knowledge_patterns = [
            "nie mam pewnych informacji",
            "nie mam informacji",
            "nie znam",
            "nie wiem",
            "nie mam danych",
            "nie mam wiedzy",
            "nie mogƒô odpowiedzieƒá",
            "nie potrafiƒô odpowiedzieƒá",
            "nie mam dostƒôpu do informacji",
            "nie mam odpowiedzi",
            "nie mam pewno≈õci",
            "nie jestem pewien",
            "nie jestem pewna",
            "nie mogƒô potwierdziƒá",
            "nie mam informacji o",
            "nie istnieje",
            "nie rozpoznajƒô",
            "nie znam takiej osoby",
            "nie znam takiego produktu",
            "nie mam danych na ten temat",
            "nie mam wiedzy na ten temat",
            "nie mogƒô udzieliƒá informacji",
            "nie potrafiƒô udzieliƒá informacji",
            "nie mam dostƒôpu do danych",
            "nie mam dostƒôpu do wiedzy",
            "nie mogƒô znale≈∫ƒá informacji",
            "nie potrafiƒô znale≈∫ƒá informacji",
            "nie mam odpowiednich informacji",
            "nie mam aktualnych informacji",
            "nie mam szczeg√≥≈Çowych informacji",
            "nie mam pe≈Çnych informacji",
            "nie mam kompletnych informacji",
            "nie mam wystarczajƒÖcych informacji",
            "nie mam dok≈Çadnych informacji",
            "nie mam precyzyjnych informacji",
            "nie mam konkretnych informacji",
            "nie mam szczeg√≥≈Çowych danych",
            "nie mam aktualnych danych",
            "nie mam odpowiednich danych",
            "nie mam kompletnych danych",
            "nie mam pe≈Çnych danych",
            "nie mam wystarczajƒÖcych danych",
            "nie mam dok≈Çadnych danych",
            "nie mam precyzyjnych danych",
            "nie mam konkretnych danych",
        ]

        # Sprawd≈∫ czy odpowied≈∫ zawiera kt√≥rykolwiek z wzorc√≥w
        for pattern in lack_of_knowledge_patterns:
            if pattern in response_lower:
                return True

        # Sprawd≈∫ czy odpowied≈∫ jest bardzo kr√≥tka (mo≈ºe wskazywaƒá na brak wiedzy)
        if len(response_text.strip()) < 50:
            # Sprawd≈∫ czy nie jest to prosty potwierdzenie lub zaprzeczenie
            simple_responses = [
                "tak",
                "nie",
                "ok",
                "dobrze",
                "rozumiem",
                "jasne",
                "zgoda",
            ]
            if response_lower.strip() not in simple_responses:
                return True

        return False

    @staticmethod
    async def _switch_to_search_mode(
        query: str, use_perplexity: bool, use_bielik: bool
    ) -> str:
        """
        Automatycznie prze≈ÇƒÖcza na tryb wyszukiwania gdy model nie zna odpowiedzi.
        """
        try:
            logger.info(f"Switching to search mode for query: {query}")

            # Utw√≥rz SearchAgent
            from agents.search_agent import SearchAgent

            search_agent = SearchAgent()

            # Przygotuj dane wej≈õciowe dla SearchAgent
            search_input = {
                "query": query,
                "model": (
                    "SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M" if use_bielik else None
                ),
                "max_results": 5,
            }

            # Wykonaj wyszukiwanie
            search_response = await search_agent.process(search_input)

            if search_response.success and search_response.text:
                # Dodaj informacjƒô o prze≈ÇƒÖczeniu na tryb wyszukiwania
                enhanced_response = (
                    f"üîç **Prze≈ÇƒÖczono na tryb wyszukiwania**\n\n"
                    f"{search_response.text}\n\n"
                    f"*Informacje zosta≈Çy znalezione w internecie.*"
                )
                return enhanced_response
            else:
                # Je≈õli wyszukiwanie nie powiod≈Ço siƒô, zwr√≥ƒá informacjƒô
                return (
                    f"Przepraszam, nie uda≈Ço mi siƒô znale≈∫ƒá odpowiednich informacji "
                    f"w internecie dla zapytania: '{query}'. "
                    f"Spr√≥buj sformu≈Çowaƒá zapytanie inaczej lub sprawd≈∫ pisowniƒô."
                )

        except Exception as e:
            logger.error(f"Error switching to search mode: {e!s}")
            return (
                "Przepraszam, wystƒÖpi≈Ç problem podczas wyszukiwania informacji. "
                "Spr√≥buj ponownie za chwilƒô."
            )

    @staticmethod
    def _determine_query_complexity(
        query: str, rag_context: str, internet_context: str
    ) -> ModelComplexity:
        """
        Okre≈õla z≈Ço≈ºono≈õƒá zapytania na podstawie jego tre≈õci i dostƒôpnego kontekstu.
        """
        query_lower = query.lower()

        # Je≈õli zapytanie jest bardzo kr√≥tkie, to jest proste
        if len(query) < 10:
            return ModelComplexity.SIMPLE

        # Je≈õli mamy du≈ºo kontekstu, to zapytanie jest z≈Ço≈ºone
        # Obs≈Çuga przypadku gdy rag_context jest tuple (str, float)
        rag_text = rag_context[0] if isinstance(rag_context, tuple) else rag_context
        combined_context = (rag_text or "") + (internet_context or "")
        if len(combined_context) > 1000:
            return ModelComplexity.COMPLEX

        # S≈Çowa kluczowe sugerujƒÖce z≈Ço≈ºone zapytanie
        complex_keywords = [
            "por√≥wnaj",
            "przeanalizuj",
            "wyja≈õnij",
            "zinterpretuj",
            "oce≈Ñ",
            "podsumuj",
            "zreferuj",
            "przedstaw argumenty",
            "uzasadnij",
            "zaprojektuj",
            "stw√≥rz",
            "napisz",
            "wymy≈õl",
            "zaproponuj",
        ]

        if any(keyword in query_lower for keyword in complex_keywords):
            return ModelComplexity.COMPLEX

        # Domy≈õlnie u≈ºywamy standardowej z≈Ço≈ºono≈õci
        return ModelComplexity.STANDARD

    @staticmethod
    def _select_model(complexity: ModelComplexity, use_bielik: bool) -> str:
        """
        Wybiera model na podstawie z≈Ço≈ºono≈õci zapytania i preferencji u≈ºytkownika
        """
        # Use only Bielik-11B for all tasks
        return "SpeakLeash/bielik-11b-v2.3-instruct:Q5_K_M"